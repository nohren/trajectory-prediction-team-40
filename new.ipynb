{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449c5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c723ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function (same as original)\n",
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # Training data with future\n",
    "        pasts, masks, futures = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        future = torch.stack(futures)\n",
    "        return past, mask, future\n",
    "    else:  # Test data without future\n",
    "        pasts, masks = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        return past, mask\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, input_path=None, data=None, T_past=50, T_future=60, is_test=False):\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        else:\n",
    "            npz = np.load(input_path)\n",
    "            self.data = npz['data']\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Calculate normalization statistics from the past data\n",
    "        self.calculate_normalization_stats()\n",
    "        \n",
    "    def calculate_normalization_stats(self):\n",
    "        \"\"\"Calculate mean and std for efficient normalization\"\"\"\n",
    "        # Only consider non-zero values for position and velocity\n",
    "        positions = self.data[..., :2]  # x, y positions\n",
    "        mask = np.abs(positions).sum(axis=-1) > 0\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            valid_positions = positions[mask]\n",
    "            self.pos_mean = valid_positions.mean(axis=0)\n",
    "            self.pos_std = valid_positions.std(axis=0)\n",
    "            \n",
    "            # Ensure std is not zero to avoid division by zero\n",
    "            self.pos_std = np.maximum(self.pos_std, 1e-6)\n",
    "        else:\n",
    "            self.pos_mean = np.zeros(2)\n",
    "            self.pos_std = np.ones(2)\n",
    "            \n",
    "        # Same for velocities\n",
    "        velocities = self.data[..., 2:4]  # vx, vy velocities\n",
    "        mask = np.abs(velocities).sum(axis=-1) > 0\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            valid_velocities = velocities[mask]\n",
    "            self.vel_mean = valid_velocities.mean(axis=0)\n",
    "            self.vel_std = valid_velocities.std(axis=0)\n",
    "            self.vel_std = np.maximum(self.vel_std, 1e-6)\n",
    "        else:\n",
    "            self.vel_mean = np.zeros(2)\n",
    "            self.vel_std = np.ones(2)\n",
    "            \n",
    "        # Heading is already in radians, so we don't normalize it\n",
    "        \n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"Normalize features efficiently\"\"\"\n",
    "        normalized = features.copy()\n",
    "        # Normalize positions (x, y)\n",
    "        normalized[..., 0:2] = (features[..., 0:2] - self.pos_mean) / self.pos_std\n",
    "        # Normalize velocities (vx, vy)\n",
    "        normalized[..., 2:4] = (features[..., 2:4] - self.vel_mean) / self.vel_std\n",
    "        # Normalize acceleration (ax, ay)\n",
    "        normalized[..., 4:6] = (features[..., 4:6] - self.vel_mean) / self.vel_std  # You might want to use vel stats for acceleration normalization\n",
    "        return normalized\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]  # (num_agents, T, 6) where 6 should be x, y, vx, vy, ax, ay\n",
    "\n",
    "        # Check the shape of `scene` to ensure it has the correct number of features\n",
    "        print(f\"Original scene shape: {scene.shape}\")  # Should be (num_agents, T, 6)\n",
    "\n",
    "        # Extract past trajectory of all agents (first T time steps)\n",
    "        past = scene[:, :self.T_past, :]  # (num_agents, T_past, 6)\n",
    "        \n",
    "        # Check the shape of `past`\n",
    "        print(f\"Shape of past: {past.shape}\")  # Should be (num_agents, T_past, 6)\n",
    "\n",
    "        # Compute velocity (dx, dy) by differentiating position\n",
    "        velocity = np.diff(past[:, :, :2], axis=1, append=np.zeros((past.shape[0], 1, 2)))  # (num_agents, T_past, 2)\n",
    "        \n",
    "        # Compute acceleration (dvx, dvy) by differentiating velocity\n",
    "        acceleration = np.diff(velocity, axis=1, append=np.zeros((past.shape[0], 1, 2)))  # (num_agents, T_past, 2)\n",
    "        \n",
    "        # Concatenate velocity and acceleration as additional features\n",
    "        past_with_derived_features = np.concatenate([past, velocity, acceleration], axis=-1)  # (num_agents, T_past, 10)\n",
    "\n",
    "        # Check the shape of `past_with_derived_features` before passing it to normalization\n",
    "        print(f\"Shape of past_with_derived_features: {past_with_derived_features.shape}\")  # Should be (num_agents, T_past, 10)\n",
    "\n",
    "        # Ensure we only have 6 features (x, y, vx, vy, ax, ay)\n",
    "        if past_with_derived_features.shape[-1] != 6:\n",
    "            # If there are more than 6 features, remove the extras\n",
    "            past_with_derived_features = past_with_derived_features[..., :6]\n",
    "            print(f\"Adjusted past_with_derived_features shape: {past_with_derived_features.shape}\")\n",
    "\n",
    "        # Normalize the features\n",
    "        past_normalized = self.normalize_features(past_with_derived_features)\n",
    "\n",
    "        # Create mask for valid agents (based on position, vx, vy)\n",
    "        mask = np.sum(np.abs(past[:, :, :2]), axis=(1, 2)) > 0\n",
    "\n",
    "        # For training data, also extract and normalize future trajectory of ego vehicle\n",
    "        if not self.is_test and scene.shape[1] >= self.T_past + self.T_future:\n",
    "            future = scene[0, self.T_past:self.T_past+self.T_future, :2]  # Ego vehicle future (x, y)\n",
    "            # Normalize future coordinates\n",
    "            future_normalized = (future - self.pos_mean) / self.pos_std\n",
    "\n",
    "            return torch.tensor(past_normalized, dtype=torch.float32), torch.tensor(mask, dtype=torch.bool), torch.tensor(future_normalized, dtype=torch.float32)\n",
    "\n",
    "        # For test data, only return normalized past\n",
    "        return torch.tensor(past_normalized, dtype=torch.float32), torch.tensor(mask, dtype=torch.bool)\n",
    "    \n",
    "    def denormalize_prediction(self, prediction):\n",
    "        \"\"\"Convert normalized predictions back to original scale\"\"\"\n",
    "        return prediction * self.pos_std + self.pos_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class AgentTypeEmbedding(nn.Module):\n",
    "    def __init__(self, num_types=10, d_model=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_types, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        obj_type = x[..., -1].long()\n",
    "        return self.embedding(obj_type)\n",
    "\n",
    "class ImprovedTrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim=6, d_model=256, nhead=8,\n",
    "                 num_layers=4, dim_feedforward=512, \n",
    "                 T_past=50, T_future=60, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        \n",
    "        # Separate embedding for features (positions, velocities, heading)\n",
    "        self.feature_embed = nn.Linear(feature_dim, d_model)  # -1 for object type\n",
    "        \n",
    "        # Object type embedding\n",
    "        self.type_embedding = AgentTypeEmbedding(num_types=10, d_model=d_model)\n",
    "        \n",
    "        # Positional encoding for timesteps\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Transformer encoder for temporal relations\n",
    "        temporal_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.temporal_encoder = nn.TransformerEncoder(\n",
    "            temporal_encoder_layer, \n",
    "            num_layers=num_layers//2\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder for social relations\n",
    "        social_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.social_encoder = nn.TransformerEncoder(\n",
    "            social_encoder_layer, \n",
    "            num_layers=num_layers//2\n",
    "        )\n",
    "        \n",
    "        # Output MLP\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, dim_feedforward // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward // 2, 2 * T_future)\n",
    "        )\n",
    "        \n",
    "    def forward(self, past, agent_mask):\n",
    "        B, N, T, F = past.shape  # Batch, Num_agents, Time, Features\n",
    "        \n",
    "        # Split features and object type\n",
    "        features = past[..., :-1]  # All but last dimension\n",
    "        \n",
    "        # Reshape to process all agent-timesteps together\n",
    "        features_flat = features.reshape(B * N * T, F-1)\n",
    "        \n",
    "        # Embed features\n",
    "        feature_embedding = self.feature_embed(features_flat)\n",
    "        feature_embedding = feature_embedding.reshape(B, N, T, self.d_model)\n",
    "        \n",
    "        # Get object type embedding\n",
    "        type_embedding = self.type_embedding(past)  # B, N, T, d_model\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_embedding = feature_embedding + type_embedding\n",
    "        \n",
    "        # Reshape for temporal transformer: (T, B*N, d_model)\n",
    "        temporal_input = combined_embedding.permute(2, 0, 1, 3).reshape(T, B*N, self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        temporal_input = self.pos_encoding(temporal_input)\n",
    "        \n",
    "        # Apply temporal transformer\n",
    "        temporal_output = self.temporal_encoder(temporal_input)\n",
    "        \n",
    "        # Get the last temporal state for each agent\n",
    "        agent_features = temporal_output[-1].reshape(B, N, self.d_model)  # B, N, d_model\n",
    "        \n",
    "        # Make sure there's at least one valid agent per batch\n",
    "        if (~agent_mask).all(dim=1).any():\n",
    "            fallback_mask = agent_mask.clone()\n",
    "            fallback_mask[:, 0] = True  # At least use ego vehicle\n",
    "            agent_mask = torch.where(agent_mask.sum(dim=1, keepdim=True) == 0, fallback_mask, agent_mask)\n",
    "        \n",
    "        # Prepare for social transformer: (N, B, d_model)\n",
    "        social_input = agent_features.permute(1, 0, 2)\n",
    "        \n",
    "        # Apply social transformer with masking\n",
    "        social_output = self.social_encoder(social_input, src_key_padding_mask=~agent_mask)\n",
    "        \n",
    "        # Extract ego vehicle embedding\n",
    "        ego_embedding = social_output[0]  # B, d_model\n",
    "        \n",
    "        # Apply prediction head\n",
    "        trajectory_flat = self.prediction_head(ego_embedding)  # B, 2*T_future\n",
    "        \n",
    "        # Reshape to (Batch, Time, XY)\n",
    "        predictions = trajectory_flat.reshape(B, self.T_future, 2)\n",
    "        \n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bf5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, clip_grad=.3):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        past, mask, future = [x.to(device) for x in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(past, mask)\n",
    "        \n",
    "        loss = criterion(pred, future)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * past.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            past, mask, future = [x.to(device) for x in batch]\n",
    "            pred = model(past, mask)\n",
    "            loss = criterion(pred, future)\n",
    "            total_loss += loss.item() * past.size(0)\n",
    "    \n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "def predict(model, test_loader, test_dataset, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            past, mask = [x.to(device) for x in batch]\n",
    "            pred = model(past, mask)\n",
    "            denorm_pred = test_dataset.denormalize_prediction(pred.cpu().numpy())\n",
    "            all_preds.append(denorm_pred)\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=0)\n",
    "\n",
    "def get_latest_checkpoint(folder):\n",
    "    files = glob.glob(os.path.join(folder, \"ckpt_epoch_*.pt\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=lambda f: int(re.findall(r\"ckpt_epoch_(\\d+)\", f)[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69aff322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "train_input = 'data/train.npz'\n",
    "test_input = 'data/test_input.npz'\n",
    "output_csv = 'predictions.csv'\n",
    "checkpoint_path = 'best_model.pt'\n",
    "checkpoints_dir = 'checkpoints'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 300\n",
    "patience = 15  # Early stopping patience\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4421e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "full_data = np.load(train_input)['data']\n",
    "\n",
    "# Split into train and validation (7:3)\n",
    "num_samples = len(full_data)\n",
    "num_train = int(0.8 * num_samples)\n",
    "perm = np.random.permutation(num_samples)\n",
    "train_idx = perm[:num_train]\n",
    "val_idx = perm[num_train:]\n",
    "\n",
    "train_data = full_data[train_idx]\n",
    "val_data = full_data[val_idx]\n",
    "\n",
    "# Create datasets with normalization\n",
    "train_ds = TrajectoryDataset(data=train_data)\n",
    "val_ds = TrajectoryDataset(data=val_data)\n",
    "\n",
    "# Create test dataset using the same normalization stats as training\n",
    "test_ds = TrajectoryDataset(input_path=test_input, is_test=True)\n",
    "# Copy normalization stats from train_ds\n",
    "test_ds.pos_mean = train_ds.pos_mean\n",
    "test_ds.pos_std = train_ds.pos_std\n",
    "test_ds.vel_mean = train_ds.vel_mean\n",
    "test_ds.vel_std = train_ds.vel_std\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621844a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmalegaonkar/trajectory-prediction-team-40/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 1\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n",
      "Original scene shape: (50, 110, 6)\n",
      "Shape of past: (50, 50, 6)\n",
      "Shape of past_with_derived_features: (50, 50, 10)\n",
      "Adjusted past_with_derived_features shape: (50, 50, 6)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 6]' is invalid for input of size 800000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, device, clip_grad)\u001b[0m\n\u001b[1;32m      7\u001b[0m past, mask, future \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, future)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/trajectory-prediction-team-40/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/trajectory-prediction-team-40/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m, in \u001b[0;36mImprovedTrajectoryTransformer.forward\u001b[0;34m(self, past, agent_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m features \u001b[38;5;241m=\u001b[39m past[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# All but the last dimension (if object type exists)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Reshape to process all agent-timesteps together (B * N * T, 6)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m features_flat \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Flatten to (B * N * T, 6)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Embed features (6 -> d_model)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m feature_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_embed(features_flat)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 6]' is invalid for input of size 800000"
     ]
    }
   ],
   "source": [
    "# Create model, optimizer, and scheduler\n",
    "model = ImprovedTrajectoryTransformer(dropout=.3).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "warm_up_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - warm_up_epochs, eta_min=1e-6)\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (epoch + 1) / warm_up_epochs if epoch < warm_up_epochs else 1)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Training setup\n",
    "start_epoch = 1\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# Try to load checkpoint\n",
    "latest_ckpt = get_latest_checkpoint(checkpoints_dir)\n",
    "if latest_ckpt:\n",
    "    print(f\"Loading checkpoint: {latest_ckpt}\")\n",
    "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt.get('val_loss', float('inf'))\n",
    "    print(f\"✅ Resumed from epoch {start_epoch - 1} with val_loss={best_val_loss:.6f}\")\n",
    "\n",
    "# Training loop\n",
    "print(f\"Starting training from epoch {start_epoch}\")\n",
    "for epoch in range(start_epoch, epochs + 1):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch <= warm_up_epochs:\n",
    "        warm_up_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss <= best_val_loss and epoch > warm_up_epochs:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✅ Best model saved at epoch {epoch} (val loss: {best_val_loss:.6f})\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss\n",
    "        }, f'{checkpoints_dir}/ckpt_epoch_{epoch:04d}.pt')\n",
    "        print(f\"🧪 Checkpoint saved at {checkpoints_dir}/ckpt_epoch_{epoch:04d}.pt\")\n",
    "    \n",
    "    # Early stopping\n",
    "    # if no_improve_epochs >= patience:\n",
    "    #     print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc077203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for prediction\n",
    "print(\"Loading best model for prediction...\")\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model_state_dict'])\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "preds = predict(model, test_loader, test_ds, device)\n",
    "\n",
    "# Flatten predictions to match submission format (2100*60, 2)\n",
    "preds_flat = preds.reshape(-1, 2)\n",
    "\n",
    "# Create ID column (required for submission)\n",
    "ids = np.arange(len(preds_flat))\n",
    "\n",
    "# Save predictions to CSV\n",
    "output = np.column_stack((ids, preds_flat))\n",
    "header = \"ID,x,y\"\n",
    "np.savetxt(output_csv, output, delimiter=',', header=header, comments='', fmt=['%d', '%.6f', '%.6f'])\n",
    "print(f\"Predictions saved to {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
