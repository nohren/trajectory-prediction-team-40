{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "449c5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a54d6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation and rotation invariance\n",
    "\n",
    "def align_future(future, center, theta):\n",
    "    xy = future - center\n",
    "    c, s = np.cos(-theta), np.sin(-theta)\n",
    "    x_new = xy[...,0]*c - xy[...,1]*s\n",
    "    y_new = xy[...,0]*s + xy[...,1]*c\n",
    "    return np.stack([x_new, y_new], axis=-1)\n",
    "\n",
    "# converts to a relative coordinate system, so model can focus on the patterns\n",
    "def invariance_transform(past, accel_dt=None):\n",
    "    \"\"\"\n",
    "    Convert a scene to an ego-centric, velocity-aligned frame.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    past : (A, T, 6) float array\n",
    "        [:,:,0:2] = x, y\n",
    "        [:,:,2:4] = vx, vy\n",
    "        [:,:,4]   = heading  (rad)\n",
    "        [:,:,5]   = type_id  (int)\n",
    "    accel_dt : float or None\n",
    "        Sampling period in seconds.  If given, an acceleration\n",
    "        channel is added (Δv / Δt) so output has 8 features.\n",
    "        If None, acceleration is omitted and output has 6 features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aligned : (A, T, 7 or 9) float array\n",
    "    center  : (2,)            ego’s last (x,y) in world frame\n",
    "    theta   : float           ego’s last heading (rad)\n",
    "    \"\"\"\n",
    "    A, T, F = past.shape\n",
    "    assert F == 6, f\"expected feat_dim = 6, got {F}\"\n",
    "\n",
    "    pos     = past[..., 0:2]          # (A,T,2)\n",
    "    vel     = past[..., 2:4]          # (A,T,2)\n",
    "    heading = past[..., 4]            # (A,T)\n",
    "    obj_id  = past[..., 5].astype(int)  # keep as float for stacking\n",
    "\n",
    "    # --- translate so ego’s last position is origin ------------------\n",
    "    center = pos[0, -1].copy()        # (2,)\n",
    "    pos_t  = pos - center             # broadcasting (A,T,2) - (2,)\n",
    "\n",
    "    # --- rotate so ego’s last heading is +X --------------------------\n",
    "    theta = heading[0, -1]            # scalar\n",
    "    c, s  = np.cos(-theta), np.sin(-theta)\n",
    "\n",
    "    # Rotate vectors\n",
    "    R = np.array([[c, -s],\n",
    "                  [s,  c]])           # 2×2\n",
    "    pos_r = pos_t @ R.T               # (A,T,2)\n",
    "    vel_r = vel   @ R.T               # (A,T,2)\n",
    "\n",
    "    # --- optional acceleration --------------------------------------\n",
    "    if accel_dt is not None:\n",
    "        inv_dt = 1.0 / accel_dt\n",
    "        acc_r = np.zeros_like(vel_r)\n",
    "        acc_r[:, 1:] = (vel_r[:, 1:] - vel_r[:, :-1]) * inv_dt\n",
    "        features = 9\n",
    "    else:\n",
    "        features = 7\n",
    "\n",
    "    # --- relative heading -------------------------------------------\n",
    "    heading_rel = ((heading - theta + np.pi) % (2*np.pi)) - np.pi  # (A,T)\n",
    "    heading_cos = np.cos(heading_rel)  # (A,T)\n",
    "    heading_sin = np.sin(heading_rel)  # (A,T)\n",
    "    heading_vec = np.stack([heading_cos, heading_sin], axis=-1)  # (A,T,2)\n",
    "\n",
    "    # --- stack output -----------------------------------------------\n",
    "    aligned = np.zeros((A, T, features), dtype=past.dtype)\n",
    "    aligned[..., 0:2] = pos_r\n",
    "    aligned[..., 2:4] = vel_r\n",
    "    if accel_dt is not None:\n",
    "        aligned[..., 4:6] = acc_r\n",
    "        aligned[..., 6:8]   = heading_vec\n",
    "        aligned[..., 8]   = obj_id\n",
    "    else:\n",
    "        aligned[..., 4:6]   = heading_vec\n",
    "        aligned[..., 6]   = obj_id\n",
    "\n",
    "    return aligned, center, theta\n",
    "\n",
    "# batch inverse transform for use outside in prediction, we only care about position inverse rotation and translation\n",
    "def inverse_transform(pred, centers, thetas):\n",
    "    \"\"\"\n",
    "    Bring aligned predictions back to world coordinates.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    pred    : (..., T, 2)  aligned positions\n",
    "    centers : (..., 2)     translation(s) subtracted in forward pass\n",
    "    thetas  : (...)        rotation angle(s) (rad), same leading dims as centers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    world : (..., T, 2)  positions in world frame\n",
    "    \"\"\"\n",
    "    pred    = np.asarray(pred)\n",
    "    centers = np.asarray(centers)\n",
    "    thetas  = np.asarray(thetas)\n",
    "\n",
    "    # Bring everything to shape (..., T, 2)\n",
    "    # Allow leading batch dims of arbitrary rank\n",
    "    # Broadcasting handles scalars automatically.\n",
    "    c = np.cos(thetas)[..., None]      # (..., 1)\n",
    "    s = np.sin(thetas)[..., None]      # (..., 1)\n",
    "\n",
    "    # Rotate back\n",
    "    x, y = pred[..., 0], pred[..., 1]\n",
    "    x_w  = x * c - y * s\n",
    "    y_w  = x * s + y * c\n",
    "    world = np.stack([x_w, y_w], axis=-1)  # (..., T, 2)\n",
    "\n",
    "    # Translate back\n",
    "    world += centers[..., None, :]         # broadcast center over T\n",
    "\n",
    "    return world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "111cd408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ forward + inverse round-trip OK\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "A, T = 5, 10\n",
    "past = np.random.randn(A, T, 6).astype(np.float32)\n",
    "past[..., 5] = np.random.randint(0, 4, size=(A, T))  # random type IDs\n",
    "\n",
    "aligned, center, theta = invariance_transform(past, accel_dt=0.1)\n",
    "pred_world = inverse_transform(aligned[0, :, :2], center, theta)  # ego track back to world\n",
    "\n",
    "assert np.allclose(pred_world, past[0, :, :2], atol=1e-5)\n",
    "print(\"✓ forward + inverse round-trip OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c723ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    l = len(batch[0])\n",
    "    if l == 3:\n",
    "        # train: (past,mask,future)\n",
    "        pasts, masks, futures = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(pasts),\n",
    "            torch.stack(masks),\n",
    "            torch.stack(futures),\n",
    "        )\n",
    "    elif l == 4:\n",
    "        # test: (past,mask,center,theta)\n",
    "        pasts, masks, centers, thetas = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(pasts),\n",
    "            torch.stack(masks),\n",
    "            torch.stack(centers),      # shape (B,2)\n",
    "            torch.stack(thetas),       # shape (B,)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized sample of length {l}\")\n",
    "    \n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, input_path=None, data=None, T_past=50, T_future=60, is_test=False):\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        else:\n",
    "            npz = np.load(input_path)\n",
    "            self.data = npz['data']\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.is_test = is_test\n",
    "\n",
    "        # Calculate normalization statistics from the past data\n",
    "        self.calculate_normalization_stats()\n",
    "\n",
    "        \n",
    "    def calculate_normalization_stats(self):\n",
    "        \"\"\"Calculate mean and std for efficient normalization\"\"\"\n",
    "        #align past data\n",
    "        all_pos = []\n",
    "        all_vel = []\n",
    "        for scene in self.data:\n",
    "            past = scene[:, :self.T_past, :].copy()\n",
    "            past_aligned, _, _ = invariance_transform(past)\n",
    "\n",
    "            # collect positions & velocities across all agents & all time-steps\n",
    "            all_pos.append(past_aligned[..., :2].reshape(-1, 2))\n",
    "            all_vel.append(past_aligned[..., 2:4].reshape(-1, 2))\n",
    "\n",
    "        all_pos = np.concatenate(all_pos, axis=0)\n",
    "        all_vel = np.concatenate(all_vel, axis=0)\n",
    "\n",
    "        # # now compute statistics on the aligned data\n",
    "        # self.pos_mean = all_pos.mean(axis=0)\n",
    "        # self.pos_std  = np.maximum(all_pos.std(axis=0), 1e-6)\n",
    "        # self.vel_mean = all_vel.mean(axis=0)\n",
    "        # self.vel_std  = np.maximum(all_vel.std(axis=0), 1e-6)\n",
    "\n",
    "        # Only consider non-zero values for position and velocity\n",
    "        #positions = self.data[..., :2]  # x, y positions\n",
    "        mask = np.abs(all_pos).sum(axis=-1) > 0\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            valid_positions = all_pos[mask]\n",
    "            self.pos_mean = valid_positions.mean(axis=0)\n",
    "            self.pos_std = valid_positions.std(axis=0)\n",
    "            \n",
    "            # Ensure std is not zero to avoid division by zero\n",
    "            self.pos_std = np.maximum(self.pos_std, 1e-6)\n",
    "        else:\n",
    "            self.pos_mean = np.zeros(2)\n",
    "            self.pos_std = np.ones(2)\n",
    "            \n",
    "        # Same for velocities\n",
    "        #velocities = self.data[..., 2:4]  # vx, vy velocities\n",
    "        mask = np.abs(all_vel).sum(axis=-1) > 0\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            valid_velocities = all_vel[mask]\n",
    "            self.vel_mean = valid_velocities.mean(axis=0)\n",
    "            self.vel_std = valid_velocities.std(axis=0)\n",
    "            self.vel_std = np.maximum(self.vel_std, 1e-6)\n",
    "        else:\n",
    "            self.vel_mean = np.zeros(2)\n",
    "            self.vel_std = np.ones(2)\n",
    "            \n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"Normalize features efficiently\"\"\"\n",
    "        normalized = features.copy()\n",
    "        # Normalize positions (x, y)\n",
    "        normalized[..., 0:2] = (features[..., 0:2] - self.pos_mean) / self.pos_std\n",
    "        # Normalize velocities (vx, vy)\n",
    "        normalized[..., 2:4] = (features[..., 2:4] - self.vel_mean) / self.vel_std\n",
    "        # Normalize acceleration (ax, ay) # it's not present RN\n",
    "        # if features.shape[-1] >= 6:  # If acceleration is present\n",
    "        #     normalized[..., 4:6] = (features[..., 4:6] - self.vel_mean) / self.vel_std\n",
    "        return normalized\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]  # (num_agents, T, features) per scene calculations\n",
    "        \n",
    "        # Extract past trajectory\n",
    "        past = scene[:, :self.T_past, :].copy()  # (num_agents, T_past, features)\n",
    "\n",
    "        \n",
    "        #TODO not only is overwriting heading and object features, but also its calculating acceleration based on velocity we are about to transform\n",
    "\n",
    "        # # If acceleration is not already in the data, calculate it\n",
    "        # if past.shape[-1] < 6:\n",
    "        #     # Original features are likely x, y, vx, vy\n",
    "        #     # Calculate acceleration from velocity if not already present\n",
    "        #     num_agents, T, _ = past.shape\n",
    "            \n",
    "        #     # Create a new array with extra space for acceleration\n",
    "        #     past_with_accel = np.zeros((num_agents, T, 6))\n",
    "            \n",
    "        #     # Copy existing features\n",
    "        #     past_with_accel[:, :, :past.shape[-1]] = past\n",
    "            \n",
    "        #     # If velocity exists, calculate acceleration as the derivative of velocity\n",
    "        #     if past.shape[-1] >= 4:  # If we have velocity\n",
    "        #         # Calculate acceleration (dvx, dvy) by differentiating velocity\n",
    "        #         accel = np.zeros((num_agents, T, 2))\n",
    "        #         accel[:, 1:, :] = past[:, 1:, 2:4] - past[:, :-1, 2:4]  # Simple finite difference\n",
    "        #         past_with_accel[:, :, 4:6] = accel\n",
    "            \n",
    "        #     past = past_with_accel\n",
    "\n",
    "        # Apply translation + rotation invariance per scene \n",
    "        # (shifts ego → origin & rotates so ego’s heading is +x)\n",
    "        # note acceleration is included in the past data this time\n",
    "        past_aligned, center, theta = invariance_transform(past)\n",
    "        \n",
    "        # Normalize features\n",
    "        past_aligned_normalized = self.normalize_features(past_aligned)\n",
    "        \n",
    "        # Create mask for valid agents (based on position)\n",
    "        mask = np.sum(np.abs(past[:, :, :2]), axis=(1, 2)) > 0\n",
    "        \n",
    "        # For training data, also extract and normalize future trajectory of ego vehicle\n",
    "        if not self.is_test and scene.shape[1] >= self.T_past + self.T_future:\n",
    "            future_raw = scene[0, self.T_past:self.T_past+self.T_future, :2]  # Ego vehicle future (x, y)\n",
    "            # align future ego to the same reference frame, then normalize\n",
    "            future_aligned = align_future(future_raw, center, theta)\n",
    "            future_aligned_normalized = (future_aligned - self.pos_mean) / self.pos_std\n",
    "            \n",
    "            return (\n",
    "                torch.tensor(past_aligned_normalized, dtype=torch.float32),\n",
    "                torch.tensor(mask, dtype=torch.bool),\n",
    "                torch.tensor(future_aligned_normalized, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        # For test data, only return aligned and normalized past\n",
    "        return (\n",
    "            torch.tensor(past_aligned_normalized, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.bool),\n",
    "            torch.tensor(center, dtype=torch.float32),\n",
    "            torch.tensor(theta, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def denormalize_prediction(self, prediction):\n",
    "        \"\"\"Convert normalized predictions back to original scale\"\"\"\n",
    "        return prediction * self.pos_std + self.pos_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "683bc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class AgentTypeEmbedding(nn.Module):\n",
    "    def __init__(self, num_types=10, d_model=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_types, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Use default type if type information is not available\n",
    "        if x.shape[-1] == 6:  # If we only have x, y, vx, vy, ax, ay, heading, object type\n",
    "            # Create default type tensor (all zeros)\n",
    "            obj_type = torch.zeros(x.shape[:-1], dtype=torch.long, device=x.device)\n",
    "        else:\n",
    "            obj_type = x[..., -1].long()\n",
    "        return self.embedding(obj_type)\n",
    "\n",
    "class ImprovedTrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim=6, d_model=256, nhead=8,\n",
    "                 num_layers=4, dim_feedforward=512, \n",
    "                 T_past=50, T_future=60, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        \n",
    "        # Feature embedding for positions, velocities, accelerations\n",
    "        self.feature_embed = nn.Linear(feature_dim, d_model)\n",
    "        \n",
    "        # Object type embedding\n",
    "        self.type_embedding = AgentTypeEmbedding(num_types=10, d_model=d_model)\n",
    "        \n",
    "        # Positional encoding for timesteps\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Transformer encoder for temporal relations\n",
    "        temporal_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.temporal_encoder = nn.TransformerEncoder(\n",
    "            temporal_encoder_layer, \n",
    "            num_layers=num_layers//2\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder for social relations\n",
    "        social_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.social_encoder = nn.TransformerEncoder(\n",
    "            social_encoder_layer, \n",
    "            num_layers=num_layers//2\n",
    "        )\n",
    "        \n",
    "        # Output MLP\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, dim_feedforward // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward // 2, 2 * T_future)\n",
    "        )\n",
    "        \n",
    "    def forward(self, past, agent_mask):\n",
    "        B, N, T, F = past.shape  # Batch, Num_agents, Time, Features\n",
    "\n",
    "        assert F >= 7, f\"Expected at least 7 features, got {F}\"\n",
    "        \n",
    "        # Embed all features directly\n",
    "        features_flat = past.reshape(B * N * T, F)\n",
    "        feature_embedding = self.feature_embed(features_flat) #project to higher space\n",
    "        feature_embedding = feature_embedding.reshape(B, N, T, self.d_model)\n",
    "        \n",
    "        # Get object type embedding\n",
    "        type_embedding = self.type_embedding(past)  # B, N, T, d_model\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_embedding = feature_embedding + type_embedding\n",
    "        \n",
    "        # Reshape for temporal transformer: (T, B*N, d_model)\n",
    "        temporal_input = combined_embedding.permute(2, 0, 1, 3).reshape(T, B*N, self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        temporal_input = self.pos_encoding(temporal_input)\n",
    "        \n",
    "        # Apply temporal transformer\n",
    "        temporal_output = self.temporal_encoder(temporal_input)\n",
    "        \n",
    "        # Get the last temporal state for each agent\n",
    "        agent_features = temporal_output[-1].reshape(B, N, self.d_model)  # B, N, d_model\n",
    "        \n",
    "        # Make sure there's at least one valid agent per batch\n",
    "        if (~agent_mask).all(dim=1).any():\n",
    "            fallback_mask = agent_mask.clone()\n",
    "            fallback_mask[:, 0] = True  # At least use ego vehicle\n",
    "            agent_mask = torch.where(agent_mask.sum(dim=1, keepdim=True) == 0, fallback_mask, agent_mask)\n",
    "        \n",
    "        # Prepare for social transformer: (N, B, d_model)\n",
    "        social_input = agent_features.permute(1, 0, 2) #want agent features back in the first dim, not time\n",
    "        \n",
    "        # Apply social transformer with masking\n",
    "        social_output = self.social_encoder(social_input, src_key_padding_mask=~agent_mask)\n",
    "        \n",
    "        # Extract ego vehicle embedding\n",
    "        ego_embedding = social_output[0]  # B, d_model\n",
    "        \n",
    "        # Apply prediction head\n",
    "        trajectory_flat = self.prediction_head(ego_embedding)  # B, 2*T_future\n",
    "        \n",
    "        # Reshape to (Batch, Time, XY)\n",
    "        predictions = trajectory_flat.reshape(B, self.T_future, 2)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69bf5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, clip_grad=.3):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        past, mask, future = [x.to(device) for x in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(past, mask)\n",
    "        \n",
    "        loss = criterion(pred, future)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * past.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            past, mask, future = [x.to(device) for x in batch]\n",
    "            pred = model(past, mask)\n",
    "            loss = criterion(pred, future)\n",
    "            total_loss += loss.item() * past.size(0)\n",
    "    \n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "def  predict(model, test_loader, test_dataset, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for past, mask, centers, thetas in test_loader:\n",
    "            past.to(device)\n",
    "            mask.to(device)\n",
    "            \n",
    "            #predict in normalized aligned space\n",
    "            pred_norm = model(past, mask)\n",
    "            pred_norm = pred_norm.cpu().numpy()\n",
    "\n",
    "            #undo normalization (still aligned)\n",
    "            pred_aligned = test_dataset.denormalize_prediction(pred_norm)\n",
    "\n",
    "            #undo relative alignment -> output world coords\n",
    "            pred_world = inverse_transform(\n",
    "                pred_aligned, centers.numpy(), thetas.numpy()\n",
    "            )\n",
    "            \n",
    "            all_preds.append(pred_world)\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=0)\n",
    "\n",
    "def get_latest_checkpoint(folder):\n",
    "    files = glob.glob(os.path.join(folder, \"ckpt_epoch_*.pt\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=lambda f: int(re.findall(r\"ckpt_epoch_(\\d+)\", f)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69aff322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "train_input = 'data/train.npz'\n",
    "test_input = 'data/test_input.npz'\n",
    "output_csv = 'predictions.csv'\n",
    "checkpoint_path = 'best_model.pt'\n",
    "checkpoints_dir = 'checkpoints'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 300\n",
    "patience = 15  # Early stopping patience\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4421e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "full_data = np.load(train_input)['data']\n",
    "\n",
    "# Split into train and validation (7:3)\n",
    "num_samples = len(full_data)\n",
    "num_train = int(0.8 * num_samples)\n",
    "perm = np.random.permutation(num_samples)\n",
    "train_idx = perm[:num_train]\n",
    "val_idx = perm[num_train:]\n",
    "\n",
    "train_data = full_data[train_idx]\n",
    "val_data = full_data[val_idx]\n",
    "\n",
    "# Create datasets with normalization\n",
    "train_ds = TrajectoryDataset(data=train_data)\n",
    "val_ds = TrajectoryDataset(data=val_data)\n",
    "\n",
    "# Create test dataset using the same normalization stats as training\n",
    "test_ds = TrajectoryDataset(input_path=test_input, is_test=True)\n",
    "# Copy normalization stats from train_ds\n",
    "test_ds.pos_mean = train_ds.pos_mean\n",
    "test_ds.pos_std = train_ds.pos_std\n",
    "test_ds.vel_mean = train_ds.vel_mean\n",
    "test_ds.vel_std = train_ds.vel_std\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a680bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45724902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: tensor([[[[-5.9821e-03, -7.3122e-03, -1.2509e-01,  ...,  1.0000e+00,\n",
      "            2.4948e-04,  0.0000e+00],\n",
      "          [-5.9821e-03, -7.3122e-03, -1.2509e-01,  ...,  1.0000e+00,\n",
      "            2.4665e-04,  0.0000e+00],\n",
      "          [-5.9821e-03, -7.3122e-03, -1.2518e-01,  ...,  1.0000e+00,\n",
      "            2.4231e-04,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.9825e-03, -7.3124e-03, -1.2514e-01,  ...,  1.0000e+00,\n",
      "            5.6571e-06,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3124e-03, -1.2515e-01,  ...,  1.0000e+00,\n",
      "            2.8240e-06,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3125e-03, -1.2515e-01,  ...,  1.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 9.5087e-03, -9.8453e-03,  1.8109e-01,  ...,  9.9559e-01,\n",
      "           -9.3787e-02,  1.0000e+00],\n",
      "          [ 9.5363e-03, -9.8485e-03,  1.8109e-01,  ...,  9.9472e-01,\n",
      "           -1.0266e-01,  1.0000e+00],\n",
      "          [ 9.5717e-03, -9.8534e-03,  1.7995e-01,  ...,  9.9352e-01,\n",
      "           -1.1368e-01,  1.0000e+00],\n",
      "          ...,\n",
      "          [ 1.2258e-02, -1.0137e-02,  2.0417e-01,  ...,  9.9116e-01,\n",
      "           -1.3270e-01,  1.0000e+00],\n",
      "          [ 1.2317e-02, -1.0143e-02,  2.0229e-01,  ...,  9.9012e-01,\n",
      "           -1.4020e-01,  1.0000e+00],\n",
      "          [ 1.2375e-02, -1.0149e-02,  1.9934e-01,  ...,  9.8935e-01,\n",
      "           -1.4553e-01,  1.0000e+00]],\n",
      "\n",
      "         [[-3.0455e-02, -3.8052e-03, -1.2508e-01,  ..., -9.9749e-01,\n",
      "            7.0806e-02,  0.0000e+00],\n",
      "          [-3.0455e-02, -3.8047e-03, -1.2508e-01,  ..., -9.9746e-01,\n",
      "            7.1175e-02,  0.0000e+00],\n",
      "          [-3.0455e-02, -3.8039e-03, -1.2509e-01,  ..., -9.9738e-01,\n",
      "            7.2306e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.0460e-02, -3.8004e-03, -1.2509e-01,  ..., -9.9800e-01,\n",
      "            6.3231e-02,  0.0000e+00],\n",
      "          [-3.0460e-02, -3.8005e-03, -1.2509e-01,  ..., -9.9802e-01,\n",
      "            6.2888e-02,  0.0000e+00],\n",
      "          [-3.0461e-02, -3.8006e-03, -1.2509e-01,  ..., -9.9803e-01,\n",
      "            6.2814e-02,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00],\n",
      "          [ 7.7402e-02, -2.9184e-01, -1.2509e-01,  ..., -4.5307e-01,\n",
      "            8.9148e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-7.8138e-03, -7.3807e-03, -1.2509e-01,  ...,  9.9752e-01,\n",
      "            7.0444e-02,  0.0000e+00],\n",
      "          [-7.8138e-03, -7.3807e-03, -1.2509e-01,  ...,  9.9752e-01,\n",
      "            7.0444e-02,  0.0000e+00],\n",
      "          [-7.8138e-03, -7.3807e-03, -1.2510e-01,  ...,  9.9752e-01,\n",
      "            7.0444e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [-6.3021e-03, -7.3149e-03,  6.7630e-01,  ...,  9.9993e-01,\n",
      "            1.2193e-02,  0.0000e+00],\n",
      "          [-6.1461e-03, -7.3132e-03,  7.2728e-01,  ...,  9.9998e-01,\n",
      "            6.1718e-03,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3125e-03,  7.6150e-01,  ...,  1.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 3.1429e-02, -5.7942e-03, -2.2566e+00,  ..., -9.9765e-01,\n",
      "           -6.8587e-02,  0.0000e+00],\n",
      "          [ 3.1212e-02, -5.7941e-03, -2.2801e+00,  ..., -9.9792e-01,\n",
      "           -6.4518e-02,  0.0000e+00],\n",
      "          [ 3.0954e-02, -5.7924e-03, -2.2854e+00,  ..., -9.9818e-01,\n",
      "           -6.0279e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 1.4095e-02, -3.4502e-03, -1.9965e+00,  ..., -9.4890e-01,\n",
      "            3.1557e-01,  0.0000e+00],\n",
      "          [ 1.3734e-02, -3.3368e-03, -1.9981e+00,  ..., -9.4793e-01,\n",
      "            3.1848e-01,  0.0000e+00],\n",
      "          [ 1.3373e-02, -3.2216e-03, -2.0032e+00,  ..., -9.4730e-01,\n",
      "            3.2034e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 2.6695e-02, -8.6300e-03, -2.1473e+00,  ..., -9.9908e-01,\n",
      "           -4.2841e-02,  0.0000e+00],\n",
      "          [ 2.6525e-02, -8.6441e-03, -2.1473e+00,  ..., -9.9931e-01,\n",
      "           -3.7015e-02,  0.0000e+00],\n",
      "          [ 2.6309e-02, -8.6610e-03, -2.2154e+00,  ..., -9.9957e-01,\n",
      "           -2.9268e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 1.5099e-02, -8.1835e-03, -4.8883e-01,  ..., -9.9645e-01,\n",
      "           -8.4179e-02,  0.0000e+00],\n",
      "          [ 1.5016e-02, -8.1885e-03, -4.4731e-01,  ..., -9.9544e-01,\n",
      "           -9.5350e-02,  0.0000e+00],\n",
      "          [ 1.4925e-02, -8.1944e-03, -4.0874e-01,  ..., -9.9512e-01,\n",
      "           -9.8683e-02,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-1.6433e-02, -8.8621e-03, -1.2509e-01,  ...,  9.6870e-01,\n",
      "            2.4822e-01,  0.0000e+00],\n",
      "          [-1.6433e-02, -8.8618e-03, -1.2509e-01,  ...,  9.6871e-01,\n",
      "            2.4820e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00],\n",
      "          [-3.8433e-01, -9.5893e-01, -1.2509e-01,  ...,  6.4998e-01,\n",
      "            7.5995e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.1153e-02, -7.2831e-03, -1.2506e-01,  ...,  1.0000e+00,\n",
      "            3.6517e-05,  0.0000e+00],\n",
      "          [-1.1154e-02, -7.2831e-03, -1.2506e-01,  ...,  1.0000e+00,\n",
      "            3.3702e-05,  0.0000e+00],\n",
      "          [-1.1154e-02, -7.2830e-03, -1.2507e-01,  ...,  1.0000e+00,\n",
      "            2.8821e-05,  0.0000e+00],\n",
      "          ...,\n",
      "          [-6.4422e-03, -7.3108e-03,  1.1014e+00,  ...,  1.0000e+00,\n",
      "           -1.1975e-03,  0.0000e+00],\n",
      "          [-6.2147e-03, -7.3117e-03,  1.1204e+00,  ...,  1.0000e+00,\n",
      "           -5.8604e-04,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3125e-03,  1.1205e+00,  ...,  1.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.2460e-03, -3.2116e-04, -1.2509e-01,  ..., -3.3382e-02,\n",
      "           -9.9944e-01,  0.0000e+00],\n",
      "          [ 1.2464e-03, -3.2129e-04, -1.2509e-01,  ..., -3.3271e-02,\n",
      "           -9.9945e-01,  0.0000e+00],\n",
      "          [ 1.2468e-03, -3.2122e-04, -1.2509e-01,  ..., -3.3050e-02,\n",
      "           -9.9945e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 1.3013e-03,  3.6475e-04, -1.2233e-01,  ..., -4.5132e-02,\n",
      "           -9.9898e-01,  0.0000e+00],\n",
      "          [ 1.3088e-03,  3.8719e-04, -1.2118e-01,  ..., -4.3311e-02,\n",
      "           -9.9906e-01,  0.0000e+00],\n",
      "          [ 1.3149e-03,  3.8178e-04, -1.2003e-01,  ..., -4.1601e-02,\n",
      "           -9.9913e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-4.7804e-02, -7.4200e-03,  2.6280e+00,  ...,  9.9999e-01,\n",
      "           -4.3202e-03,  0.0000e+00],\n",
      "          [-4.7660e-02, -7.4181e-03,  2.6281e+00,  ...,  1.0000e+00,\n",
      "           -1.4164e-03,  0.0000e+00],\n",
      "          [-4.7458e-02, -7.4157e-03,  2.6445e+00,  ...,  1.0000e+00,\n",
      "            1.5454e-03,  0.0000e+00],\n",
      "          ...,\n",
      "          [-2.3941e-02, -7.2652e-03,  2.7760e+00,  ...,  9.9996e-01,\n",
      "            9.2406e-03,  0.0000e+00],\n",
      "          [-2.3408e-02, -7.2602e-03,  2.7676e+00,  ...,  9.9993e-01,\n",
      "            1.1482e-02,  0.0000e+00],\n",
      "          [-2.2875e-02, -7.2558e-03,  2.7592e+00,  ...,  9.9992e-01,\n",
      "            1.2596e-02,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [ 6.1484e-02, -8.8281e-03,  2.5476e+00,  ...,  9.0332e-01,\n",
      "            4.2896e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 6.1528e-02, -7.7659e-03,  3.1381e+00,  ...,  9.1597e-01,\n",
      "            4.0124e-01,  0.0000e+00],\n",
      "          [ 6.2323e-02, -7.6537e-03,  3.1512e+00,  ...,  9.1969e-01,\n",
      "            3.9264e-01,  0.0000e+00],\n",
      "          [ 6.3107e-02, -7.5451e-03,  3.1643e+00,  ...,  9.2357e-01,\n",
      "            3.8342e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00],\n",
      "          [-5.7875e-01,  1.0980e-01, -1.2509e-01,  ...,  9.4349e-01,\n",
      "           -3.3141e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.5720e-02, -6.7449e-03, -1.2509e-01,  ...,  9.9915e-01,\n",
      "           -4.1141e-02,  0.0000e+00],\n",
      "          [-2.5561e-02, -6.7516e-03, -1.2509e-01,  ...,  9.9916e-01,\n",
      "           -4.1093e-02,  0.0000e+00],\n",
      "          [-2.5354e-02, -6.7603e-03,  1.0137e+00,  ...,  9.9916e-01,\n",
      "           -4.1033e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [-6.8041e-03, -7.3127e-03,  2.1124e+00,  ...,  1.0000e+00,\n",
      "           -6.7029e-04,  0.0000e+00],\n",
      "          [-6.3921e-03, -7.3127e-03,  2.1006e+00,  ...,  1.0000e+00,\n",
      "           -2.7981e-04,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3125e-03,  2.1118e+00,  ...,  1.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 2.5257e-03, -5.5820e-03, -1.2509e-01,  ..., -9.9916e-01,\n",
      "            4.1077e-02,  0.0000e+00],\n",
      "          [ 2.5918e-03, -5.5575e-03, -1.2509e-01,  ..., -9.9916e-01,\n",
      "            4.1069e-02,  0.0000e+00],\n",
      "          [ 2.6737e-03, -5.5310e-03, -1.2509e-01,  ..., -9.9916e-01,\n",
      "            4.1046e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 3.1189e-03, -5.4053e-03, -1.2509e-01,  ..., -9.9922e-01,\n",
      "            3.9597e-02,  0.0000e+00],\n",
      "          [ 3.1191e-03, -5.4056e-03, -1.2509e-01,  ..., -9.9921e-01,\n",
      "            3.9714e-02,  0.0000e+00],\n",
      "          [ 3.1194e-03, -5.4062e-03, -1.2509e-01,  ..., -9.9920e-01,\n",
      "            3.9877e-02,  0.0000e+00]],\n",
      "\n",
      "         [[-5.3167e-04, -5.2523e-03, -1.2509e-01,  ..., -9.9940e-01,\n",
      "            3.4683e-02,  0.0000e+00],\n",
      "          [-5.3123e-04, -5.2525e-03, -1.2509e-01,  ..., -9.9940e-01,\n",
      "            3.4722e-02,  0.0000e+00],\n",
      "          [-5.3098e-04, -5.2530e-03, -1.2509e-01,  ..., -9.9940e-01,\n",
      "            3.4748e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.6231e-04, -5.3403e-03, -1.2509e-01,  ..., -9.9928e-01,\n",
      "            3.8043e-02,  0.0000e+00],\n",
      "          [-5.6177e-04, -5.3412e-03, -1.2509e-01,  ..., -9.9928e-01,\n",
      "            3.7983e-02,  0.0000e+00],\n",
      "          [-5.6107e-04, -5.3421e-03, -1.2509e-01,  ..., -9.9928e-01,\n",
      "            3.7963e-02,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-8.3326e-03,  1.1997e-02,  2.8028e-01,  ..., -4.9076e-02,\n",
      "           -9.9880e-01,  0.0000e+00],\n",
      "          [-8.3530e-03,  1.1913e-02,  2.9632e-01,  ..., -5.0182e-02,\n",
      "           -9.9874e-01,  0.0000e+00],\n",
      "          [-8.3783e-03,  1.1854e-02,  2.7210e-01,  ..., -5.1218e-02,\n",
      "           -9.9869e-01,  0.0000e+00]],\n",
      "\n",
      "         [[-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          [-1.8436e+00,  1.4876e+00, -1.2509e-01,  ...,  5.3277e-01,\n",
      "           -8.4626e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.5904e-03,  1.4627e-02, -1.2378e-01,  ...,  2.2501e-02,\n",
      "            9.9975e-01,  0.0000e+00],\n",
      "          [-5.5836e-03,  1.4629e-02, -1.2326e-01,  ...,  2.2612e-02,\n",
      "            9.9974e-01,  0.0000e+00],\n",
      "          [-5.5769e-03,  1.4630e-02, -1.2129e-01,  ...,  2.2768e-02,\n",
      "            9.9974e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.9827e-03, -7.3122e-03, -1.2507e-01,  ...,  1.0000e+00,\n",
      "           -1.0421e-05,  0.0000e+00],\n",
      "          [-5.9827e-03, -7.3122e-03, -1.2507e-01,  ...,  1.0000e+00,\n",
      "           -1.0336e-05,  0.0000e+00],\n",
      "          [-5.9827e-03, -7.3122e-03, -1.2508e-01,  ...,  1.0000e+00,\n",
      "           -1.0229e-05,  0.0000e+00],\n",
      "          ...,\n",
      "          [-5.9825e-03, -7.3124e-03, -1.2508e-01,  ...,  1.0000e+00,\n",
      "           -2.2030e-07,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3124e-03, -1.2507e-01,  ...,  1.0000e+00,\n",
      "           -8.7721e-08,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3125e-03, -1.2508e-01,  ...,  1.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[-2.2764e-02,  5.4442e-03,  1.6539e-01,  ...,  4.6228e-01,\n",
      "           -8.8673e-01,  0.0000e+00],\n",
      "          [-2.2662e-02,  5.3376e-03,  4.1449e-01,  ...,  4.6111e-01,\n",
      "           -8.8734e-01,  0.0000e+00],\n",
      "          [-2.2539e-02,  5.1983e-03,  4.9932e-01,  ...,  4.6105e-01,\n",
      "           -8.8738e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.7117e-02, -6.4813e-03,  6.1795e-01,  ...,  4.3372e-01,\n",
      "           -9.0105e-01,  0.0000e+00],\n",
      "          [-1.6987e-02, -6.7805e-03,  6.0752e-01,  ...,  4.2624e-01,\n",
      "           -9.0461e-01,  0.0000e+00],\n",
      "          [-1.6859e-02, -7.0798e-03,  6.0680e-01,  ...,  4.1858e-01,\n",
      "           -9.0818e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.1654e-03,  4.4280e-03, -4.7281e-01,  ..., -9.4418e-01,\n",
      "            3.2944e-01,  0.0000e+00],\n",
      "          [ 1.1162e-03,  4.4513e-03, -4.2857e-01,  ..., -9.4379e-01,\n",
      "            3.3055e-01,  0.0000e+00],\n",
      "          [ 1.0532e-03,  4.4813e-03, -4.0036e-01,  ..., -9.4326e-01,\n",
      "            3.3206e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-8.7031e-03,  8.6118e-03, -1.6430e+00,  ..., -9.3164e-01,\n",
      "            3.6338e-01,  0.0000e+00],\n",
      "          [-8.9985e-03,  8.7429e-03, -1.6951e+00,  ..., -9.3059e-01,\n",
      "            3.6606e-01,  0.0000e+00],\n",
      "          [-9.2931e-03,  8.8721e-03, -1.7411e+00,  ..., -9.3011e-01,\n",
      "            3.6727e-01,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 5.9509e-03,  3.9313e-03, -1.6003e+00,  ..., -9.1490e-01,\n",
      "            4.0368e-01,  0.0000e+00],\n",
      "          [ 5.8251e-03,  3.9937e-03, -1.5502e+00,  ..., -9.1414e-01,\n",
      "            4.0539e-01,  0.0000e+00],\n",
      "          [ 5.6405e-03,  4.0835e-03, -1.5095e+00,  ..., -9.1371e-01,\n",
      "            4.0638e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00],\n",
      "          [ 2.2622e+00,  3.4132e-01, -1.2509e-01,  ..., -9.2367e-01,\n",
      "            3.8320e-01,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.1121e-02, -7.3941e-03,  1.1600e+00,  ...,  9.9945e-01,\n",
      "            3.3138e-02,  0.0000e+00],\n",
      "          [-2.0989e-02, -7.3898e-03,  1.1600e+00,  ...,  9.9947e-01,\n",
      "            3.2462e-02,  0.0000e+00],\n",
      "          [-2.0829e-02, -7.3848e-03,  1.1549e+00,  ...,  9.9950e-01,\n",
      "            3.1776e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [-6.8081e-03, -7.3125e-03,  2.1018e+00,  ...,  1.0000e+00,\n",
      "           -4.7362e-04,  0.0000e+00],\n",
      "          [-6.3960e-03, -7.3125e-03,  2.1018e+00,  ...,  1.0000e+00,\n",
      "           -2.9019e-04,  0.0000e+00],\n",
      "          [-5.9825e-03, -7.3125e-03,  2.1008e+00,  ...,  1.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.2959e-02, -9.7606e-03, -1.6564e-01,  ...,  9.9991e-01,\n",
      "            1.3129e-02,  0.0000e+00],\n",
      "          [ 1.2956e-02, -9.7603e-03, -1.6564e-01,  ...,  9.9992e-01,\n",
      "            1.3025e-02,  0.0000e+00],\n",
      "          [ 1.2952e-02, -9.7597e-03, -1.6564e-01,  ...,  9.9992e-01,\n",
      "            1.2892e-02,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 1.2959e-02, -9.7329e-03, -1.2509e-01,  ...,  9.9995e-01,\n",
      "            9.7296e-03,  0.0000e+00],\n",
      "          [ 1.2960e-02, -9.7326e-03, -1.2509e-01,  ...,  9.9994e-01,\n",
      "            1.0833e-02,  0.0000e+00],\n",
      "          [ 1.2960e-02, -9.7327e-03, -1.2509e-01,  ...,  9.9993e-01,\n",
      "            1.1991e-02,  0.0000e+00]],\n",
      "\n",
      "         [[ 2.8664e-03, -1.2571e-05, -2.3480e-01,  ..., -9.9147e-01,\n",
      "            1.3037e-01,  1.0000e+00],\n",
      "          [ 2.8292e-03, -8.6070e-06, -3.2709e-01,  ..., -9.9055e-01,\n",
      "            1.3712e-01,  1.0000e+00],\n",
      "          [ 2.7875e-03, -4.4280e-06, -3.7508e-01,  ..., -9.8936e-01,\n",
      "            1.4549e-01,  1.0000e+00],\n",
      "          ...,\n",
      "          [ 6.0330e-04, -3.8299e-06, -3.7551e-01,  ..., -9.9624e-01,\n",
      "           -8.6606e-02,  1.0000e+00],\n",
      "          [ 5.5594e-04, -4.5216e-06, -3.7242e-01,  ..., -9.9713e-01,\n",
      "           -7.5705e-02,  1.0000e+00],\n",
      "          [ 5.0698e-04, -6.6477e-06, -3.6933e-01,  ..., -9.9785e-01,\n",
      "           -6.5600e-02,  1.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-8.6167e-04,  1.2199e-02, -1.0833e-01,  ..., -4.7836e-02,\n",
      "            9.9886e-01,  0.0000e+00],\n",
      "          [-8.6502e-04,  1.2198e-02, -1.0962e-01,  ..., -4.7992e-02,\n",
      "            9.9885e-01,  0.0000e+00],\n",
      "          [-8.6908e-04,  1.2198e-02, -1.1213e-01,  ..., -4.7961e-02,\n",
      "            9.9885e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00]],\n",
      "\n",
      "         [[ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          [ 5.7249e-01, -2.6617e+00, -1.2509e-01,  ...,  2.9290e-02,\n",
      "            9.9957e-01,  0.0000e+00],\n",
      "          ...,\n",
      "          [-1.8161e-03, -2.3661e-02, -1.2509e-01,  ..., -1.1376e-01,\n",
      "           -9.9351e-01,  0.0000e+00],\n",
      "          [-1.8126e-03, -2.3633e-02, -1.2508e-01,  ..., -1.1875e-01,\n",
      "           -9.9292e-01,  0.0000e+00],\n",
      "          [-1.8125e-03, -2.3605e-02, -1.2509e-01,  ..., -1.2156e-01,\n",
      "           -9.9258e-01,  0.0000e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(train_loader))\n",
    "\n",
    "print(\"First sample:\", b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "621844a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 1\n",
      "Epoch 1/300 | Train Loss: 0.001127 | Val Loss: 0.000361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[32m     32\u001b[39m     val_loss = evaluate(model, val_loader, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, clip_grad)\u001b[39m\n\u001b[32m     15\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n\u001b[32m     17\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * past.size(\u001b[32m0\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create model, optimizer, and scheduler\n",
    "model = ImprovedTrajectoryTransformer(feature_dim=7, dropout=.3).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
    "warm_up_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs - warm_up_epochs, eta_min=1e-6)\n",
    "warm_up_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (epoch + 1) / warm_up_epochs if epoch < warm_up_epochs else 1)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "# Training setup\n",
    "start_epoch = 1\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# Try to load checkpoint\n",
    "latest_ckpt = get_latest_checkpoint(checkpoints_dir)\n",
    "if latest_ckpt:\n",
    "    print(f\"Loading checkpoint: {latest_ckpt}\")\n",
    "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt.get('val_loss', float('inf'))\n",
    "    print(f\"✅ Resumed from epoch {start_epoch - 1} with val_loss={best_val_loss:.6f}\")\n",
    "\n",
    "# Training loop\n",
    "print(f\"Starting training from epoch {start_epoch}\")\n",
    "for epoch in range(start_epoch, epochs + 1):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch <= warm_up_epochs:\n",
    "        warm_up_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss <= best_val_loss and epoch > warm_up_epochs:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✅ Best model saved at epoch {epoch} (val loss: {best_val_loss:.6f})\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "    \n",
    "    # Save periodic checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss\n",
    "        }, f'{checkpoints_dir}/ckpt_epoch_{epoch:04d}.pt')\n",
    "        print(f\"🧪 Checkpoint saved at {checkpoints_dir}/ckpt_epoch_{epoch:04d}.pt\")\n",
    "    \n",
    "    # Early stopping\n",
    "    # if no_improve_epochs >= patience:\n",
    "    #     print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc077203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for prediction...\n",
      "Generating predictions...\n",
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Load best model for prediction\n",
    "print(\"Loading best model for prediction...\")\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model_state_dict'])\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "preds = predict(model, test_loader, test_ds, device)\n",
    "\n",
    "# Flatten predictions to match submission format (2100*60, 2)\n",
    "preds_flat = preds.reshape(-1, 2)\n",
    "\n",
    "# Create ID column (required for submission)\n",
    "ids = np.arange(len(preds_flat))\n",
    "\n",
    "# Save predictions to CSV\n",
    "output = np.column_stack((ids, preds_flat))\n",
    "header = \"ID,x,y\"\n",
    "np.savetxt(output_csv, output, delimiter=',', header=header, comments='', fmt=['%d', '%.6f', '%.6f'])\n",
    "print(f\"Predictions saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8767159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past XY reconstruction max‐error: 2.842e-14\n",
      "Future invariance max‐error: 0.000e+00\n",
      "Future norm/denorm max‐error: 7.105e-15\n",
      "Batch shapes:\n",
      "  past: torch.Size([8, 50, 50, 7])\n",
      "  mask: torch.Size([8, 50])\n",
      "  centers: torch.Size([8, 2])\n",
      "  thetas: torch.Size([8])\n",
      "  pred_world: (8, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# assume: TrajectoryDataset, invariance_transform, inverse_transform,\n",
    "#         align_future, collate_fn, etc. are already defined above\n",
    "\n",
    "ds = TrajectoryDataset(input_path='data/train.npz', is_test=False)\n",
    "\n",
    "# pick one scene\n",
    "idx = 0\n",
    "scene = ds.data[idx]\n",
    "\n",
    "# 1) PAST reconstruction test\n",
    "# — slice ALL features, not just :2\n",
    "raw_past_feats = scene[:, :ds.T_past, :].copy()           # (A, T_past, F>=4)\n",
    "aligned_feats, center, theta = invariance_transform(raw_past_feats) #one aligned scene\n",
    "\n",
    "# now inverse only the XY channels\n",
    "recon_xy = inverse_transform(aligned_feats[..., :2], center, theta)\n",
    "err_past = np.max(np.abs(recon_xy - raw_past_feats[..., :2]))\n",
    "print(f'Past XY reconstruction max‐error: {err_past:.3e}')\n",
    "\n",
    "if scene.shape[1] >= ds.T_past + ds.T_future:\n",
    "    raw_fut = scene[0, ds.T_past:ds.T_past+ds.T_future, :2].copy()\n",
    "\n",
    "    # 1) align → inverse (should reconstruct raw_fut up to numerical noise)\n",
    "    fut_aln   = align_future(raw_fut, center, theta)\n",
    "    recon_if  = inverse_transform(fut_aln, center, theta)\n",
    "    err_if    = np.max(np.abs(recon_if - raw_fut))\n",
    "    print(f'Future invariance max‐error: {err_if:.3e}')\n",
    "\n",
    "    # 2) normalization → denormalization (should also be tiny)\n",
    "    fut_norm  = (fut_aln - ds.pos_mean) / ds.pos_std\n",
    "    fut_den   = ds.denormalize_prediction(fut_norm)\n",
    "    err_scale = np.max(np.abs(fut_den - fut_aln))\n",
    "    print(f'Future norm/denorm max‐error: {err_scale:.3e}')\n",
    "\n",
    "# 3) BATCH shapes test\n",
    "ds_test = TrajectoryDataset(input_path='data/train.npz', is_test=True)\n",
    "loader  = DataLoader(ds_test, batch_size=8, collate_fn=collate_fn)\n",
    "past_b, mask_b, centers_b, thetas_b = next(iter(loader))\n",
    "print('Batch shapes:')\n",
    "print('  past:',     past_b.shape)       # (8, A, T_past, F)\n",
    "print('  mask:',     mask_b.shape)       # (8, A)\n",
    "print('  centers:',  centers_b.shape)    # (8, 2)\n",
    "print('  thetas:',   thetas_b.shape)     # (8,)\n",
    "\n",
    "# quick pipeline shape‐check\n",
    "pred_norm = np.random.randn(8, ds.T_future, 2)\n",
    "pred_aln  = ds.denormalize_prediction(pred_norm)\n",
    "pred_w    = inverse_transform(pred_aln, centers_b.numpy(), thetas_b.numpy())\n",
    "print('  pred_world:', pred_w.shape)    # (8, T_future, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7762b24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
