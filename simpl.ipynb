{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca95b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a1a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # Training data with future\n",
    "        pasts, masks, futures = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        future = torch.stack(futures)\n",
    "        return past, mask, future\n",
    "    else:  # Test data without future\n",
    "        pasts, masks = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        return past, mask\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, input_path=None, data=None, T_past=50, T_future=60, is_test=False):\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        else:\n",
    "            npz = np.load(input_path)\n",
    "            self.data = npz['data']\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]  # (num_agents, T, 6)\n",
    "        past = scene[:, :self.T_past, :]\n",
    "        mask = np.sum(np.abs(past[..., :2]), axis=(1, 2)) > 0\n",
    "        if not self.is_test and scene.shape[1] >= self.T_past + self.T_future:\n",
    "            future = scene[0, self.T_past:self.T_past + self.T_future, :2]\n",
    "            return (\n",
    "                torch.tensor(past, dtype=torch.float32),\n",
    "                torch.tensor(mask, dtype=torch.bool),\n",
    "                torch.tensor(future, dtype=torch.float32)\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(past, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.bool)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a741e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernsteinLayer(nn.Module):\n",
    "    def __init__(self, n_degree, T=60):\n",
    "        super().__init__()\n",
    "        self.n_degree = n_degree\n",
    "        self.T = T\n",
    "        self._precompute_bernstein_values()\n",
    "\n",
    "    def _precompute_bernstein_values(self):\n",
    "        time_points = torch.linspace(0, 1, self.T)\n",
    "        bern = torch.zeros(self.n_degree + 1, self.T)\n",
    "        for i in range(self.n_degree + 1):\n",
    "            binom = math.comb(self.n_degree, i)\n",
    "            bern[i] = binom * (time_points ** i) * ((1 - time_points) ** (self.n_degree - i))\n",
    "        # register as (n+1, T)\n",
    "        self.register_buffer('bernstein_values', bern)\n",
    "\n",
    "    def forward(self, control_points):\n",
    "        # control_points: (B, n+1, 2)\n",
    "        cp = control_points.transpose(1, 2)  # (B, 2, n+1)\n",
    "        # matmul broadcasts: (B,2,n+1) @ (n+1,T) -> (B,2,T)\n",
    "        traj = torch.matmul(cp, self.bernstein_values)  # (B,2,T)\n",
    "        trajectories = traj.transpose(1, 2)  # (B,T,2)\n",
    "        return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0075f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pos_enc = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        # shape (1, max_len, d_model)\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pos_enc[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8f59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rpe_processor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, source, target, rpe=None):\n",
    "        B, N_source, D = source.shape\n",
    "        _, N_target, _ = target.shape\n",
    "        query = self.query_proj(target)\n",
    "        key = self.key_proj(source)\n",
    "        value = self.value_proj(source)\n",
    "        if rpe is not None:\n",
    "            proc = self.rpe_processor(rpe)\n",
    "            enhanced = key.unsqueeze(1).repeat(1, N_target, 1, 1) + proc\n",
    "            outputs = []\n",
    "            for i in range(N_target):\n",
    "                q = query[:, i:i+1, :]\n",
    "                k = enhanced[:, i, :, :]\n",
    "                v = value\n",
    "                out, _ = self.multihead_attn(q, k, v)\n",
    "                outputs.append(out)\n",
    "            attn_output = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        target = self.norm1(target + self.dropout(attn_output))\n",
    "        ff_out = self.ff(target)\n",
    "        return self.norm2(target + self.dropout(ff_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0a6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIMPLModel(nn.Module):\n",
    "    def __init__(self, feature_dim=6, d_model=128, nhead=8,\n",
    "                 num_layers_temporal=2, num_layers_social=2,\n",
    "                 dim_feedforward=256, T_past=50, T_future=60,\n",
    "                 polynomial_degree=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.input_embed = nn.Linear(feature_dim, d_model)\n",
    "        self.time_pos_enc = PositionalEncoding(d_model, max_len=T_past)\n",
    "        self.temporal_encoders = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "            for _ in range(num_layers_temporal)\n",
    "        ])\n",
    "        self.rpe_generator = nn.Sequential(\n",
    "            nn.Linear(feature_dim, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, d_model)\n",
    "        )\n",
    "        self.social_encoders = nn.ModuleList([\n",
    "            SymmetricAttention(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers_social)\n",
    "        ])\n",
    "        self.control_point_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, dim_feedforward // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward // 2, 2 * (polynomial_degree + 1))\n",
    "        )\n",
    "        self.bernstein_layer = BernsteinLayer(polynomial_degree, T_future)\n",
    "\n",
    "    def compute_relative_position_embedding(self, past, mask):\n",
    "        B, N, T, F = past.shape\n",
    "        last = past[:, :, -1, :]\n",
    "        ego = last[:, 0:1, :].expand(-1, N, -1)\n",
    "        rpe_feats = ego - last\n",
    "        rpe_feats = rpe_feats * mask.unsqueeze(-1).float()\n",
    "        return self.rpe_generator(rpe_feats)\n",
    "\n",
    "    def forward(self, past, agent_mask):\n",
    "        B, N, T, F = past.shape\n",
    "        # always include ego agent\n",
    "        agent_mask = agent_mask.clone()\n",
    "        agent_mask[:, 0] = True\n",
    "        # temporal embedding\n",
    "        x = past.view(B * N, T, F)\n",
    "        x = self.input_embed(x)\n",
    "        x = x / (x.norm(dim=-1, keepdim=True) + 1e-6) * math.sqrt(self.d_model)\n",
    "        x = self.time_pos_enc(x)\n",
    "        for layer in self.temporal_encoders:\n",
    "            x = layer(x)\n",
    "        # pool final state\n",
    "        x = x[:, -1, :]\n",
    "        agent_feats = x.view(B, N, self.d_model)\n",
    "        # social interaction\n",
    "        rpe = self.compute_relative_position_embedding(past, agent_mask)\n",
    "        ego_feats = agent_feats[:, 0:1, :]\n",
    "        others = agent_feats\n",
    "        rpe_mat = rpe.unsqueeze(1)\n",
    "        for layer in self.social_encoders:\n",
    "            ego_feats = layer(others, ego_feats, rpe_mat)\n",
    "        ego_embed = ego_feats.squeeze(1)\n",
    "        cps_flat = self.control_point_predictor(ego_embed)\n",
    "        cps = cps_flat.view(B, self.polynomial_degree + 1, 2)\n",
    "        return self.bernstein_layer(cps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a2d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device, num_epochs=10, lr_scheduler=None, writer=None, global_step=0):\n",
    "    model.train()\n",
    "    position_criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        past, mask, future = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Check for NaNs or Infs in inputs\n",
    "        if torch.isnan(past).any() or torch.isinf(past).any():\n",
    "            print(\"Warning: NaN or Inf detected in past input. Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        if torch.isnan(future).any() or torch.isinf(future).any():\n",
    "            print(\"Warning: NaN or Inf detected in future target. Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(past, mask)\n",
    "        \n",
    "        # Check for NaNs or Infs in predictions\n",
    "        if torch.isnan(pred).any() or torch.isinf(pred).any():\n",
    "            print(\"Warning: NaN or Inf detected in predictions. Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        # Position loss (smooth L1)\n",
    "        loss = position_criterion(pred, future)\n",
    "        \n",
    "        # Calculate ADE for monitoring\n",
    "        with torch.no_grad():\n",
    "            mse = torch.pow(pred - future, 2).sum(dim=2)  # (B, T)\n",
    "            ade = torch.sqrt(mse).mean(dim=1).mean()  # scalar\n",
    "            fde = torch.sqrt(mse[:, -1]).mean()  # scalar\n",
    "        \n",
    "        # Backward pass with gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * past.size(0)\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Log metrics to tensorboard every 20 batches\n",
    "        if writer is not None and batch_idx % 20 == 0:\n",
    "            writer.add_scalar('train/batch_loss', batch_loss, global_step)\n",
    "            writer.add_scalar('train/batch_ade', ade.item(), global_step)\n",
    "            writer.add_scalar('train/batch_fde', fde.item(), global_step)\n",
    "            writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "            \n",
    "            \n",
    "            # Log histograms of model weights and gradients\n",
    "            if batch_idx % 200 == 0:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        writer.add_histogram(f'weights/{name}', param.data, global_step)\n",
    "                        if param.grad is not None:\n",
    "                            writer.add_histogram(f'gradients/{name}', param.grad, global_step)\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    # Step learning rate scheduler if provided\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader.dataset)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('train/epoch_loss', epoch_loss, global_step)\n",
    "        writer.add_scalar('train/epoch_time', epoch_time, global_step)\n",
    "    \n",
    "    print(f\"Training - Loss: {epoch_loss:.6f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return model, global_step\n",
    "\n",
    "# Evaluation function with tensorboard logging\n",
    "def evaluate(model, val_loader, device, writer=None, global_step=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    mse_criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    all_errors = []\n",
    "    all_ades = []\n",
    "    all_fdes = []\n",
    "    \n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            past, mask, future = [x.to(device) for x in batch]\n",
    "            pred = model(past, mask)\n",
    "            \n",
    "            # Calculate MSE loss per time step and sample\n",
    "            mse = mse_criterion(pred, future)  # (B, T, 2)\n",
    "            \n",
    "            # Calculate ADE (Average Displacement Error)\n",
    "            ade = torch.sqrt(mse.sum(dim=2)).mean(dim=1)  # (B,)\n",
    "            all_ades.extend(ade.cpu().numpy())\n",
    "            \n",
    "            # Calculate FDE (Final Displacement Error)\n",
    "            fde = torch.sqrt(mse[:, -1].sum(dim=1))  # (B,)\n",
    "            all_fdes.extend(fde.cpu().numpy())\n",
    "            \n",
    "            # Store errors for reporting\n",
    "            for i in range(len(ade)):\n",
    "                all_errors.append({\n",
    "                    'ade': ade[i].item(),\n",
    "                    'fde': fde[i].item()\n",
    "                })\n",
    "            \n",
    "            # Accumulate MSE loss\n",
    "            batch_loss = mse.mean()\n",
    "            total_loss += batch_loss.item() * past.size(0)\n",
    "            \n",
    "            # Log sample trajectories periodically\n",
    "            if writer is not None and batch_idx % 50 == 0 and batch_idx < 150:\n",
    "                # Plot sample trajectories for validation\n",
    "                for i in range(min(2, past.size(0))):\n",
    "                    fig_path = f'val_sample_{i}_step_{global_step}.png'\n",
    "                    # In a real implementation, you'd visualize trajectories here\n",
    "                    # writer.add_figure(f'val/trajectory_{i}', fig, global_step)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    avg_ade = np.mean(all_ades)\n",
    "    avg_fde = np.mean(all_fdes)\n",
    "    \n",
    "    # Calculate additional metrics - percentiles\n",
    "    ade_50 = np.percentile(all_ades, 50)  # median\n",
    "    ade_90 = np.percentile(all_ades, 90)  # 90th percentile\n",
    "    fde_50 = np.percentile(all_fdes, 50)  # median\n",
    "    fde_90 = np.percentile(all_fdes, 90)  # 90th percentile\n",
    "    \n",
    "    eval_time = time.time() - eval_start_time\n",
    "    \n",
    "    # Log metrics to tensorboard\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar('val/loss', avg_loss, global_step)\n",
    "        writer.add_scalar('val/ade_mean', avg_ade, global_step)\n",
    "        writer.add_scalar('val/fde_mean', avg_fde, global_step)\n",
    "        writer.add_scalar('val/ade_50', ade_50, global_step)\n",
    "        writer.add_scalar('val/ade_90', ade_90, global_step)\n",
    "        writer.add_scalar('val/fde_50', fde_50, global_step)\n",
    "        writer.add_scalar('val/fde_90', fde_90, global_step)\n",
    "        writer.add_scalar('val/eval_time', eval_time, global_step)\n",
    "        \n",
    "        # Add histograms of ADE and FDE\n",
    "        writer.add_histogram('val/ade_dist', np.array(all_ades), global_step)\n",
    "        writer.add_histogram('val/fde_dist', np.array(all_fdes), global_step)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'ade': avg_ade,\n",
    "        'fde': avg_fde,\n",
    "        'ade_50': ade_50,\n",
    "        'ade_90': ade_90,\n",
    "        'fde_50': fde_50,\n",
    "        'fde_90': fde_90,\n",
    "        'eval_time': eval_time,\n",
    "        'detailed_errors': all_errors\n",
    "    }\n",
    "\n",
    "# Prediction function with optional tensorboard visualizations\n",
    "def predict(model, test_loader, device, writer=None, visualize_samples=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    inference_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            past, mask = [x.to(device) for x in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(past, mask)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "            # Visualize sample predictions (only if requested and writer is provided)\n",
    "            if writer is not None and visualize_samples and batch_idx < 10:\n",
    "                # In a real implementation, you would generate and save figures\n",
    "                # writer.add_figure(f'test/trajectory_{batch_idx}', fig, 0)\n",
    "                pass\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    \n",
    "    # Log inference statistics if writer is provided\n",
    "    if writer is not None:\n",
    "        inference_time = time.time() - inference_start_time\n",
    "        avg_time_per_sample = inference_time / len(predictions)\n",
    "        writer.add_text('inference_stats', \n",
    "                      f\"Total inference time: {inference_time:.2f}s, \"\n",
    "                      f\"Samples: {len(predictions)}, \"\n",
    "                      f\"Avg time per sample: {avg_time_per_sample*1000:.2f}ms\")\n",
    "        \n",
    "        # Add histogram of prediction coordinates\n",
    "        writer.add_histogram('test/pred_x', predictions[:, :, 0].flatten(), 0)\n",
    "        writer.add_histogram('test/pred_y', predictions[:, :, 1].flatten(), 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e14e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = 'data/train.npz'\n",
    "test_input  = 'data/test_input.npz'\n",
    "output_csv  = 'predictions.csv'\n",
    "\n",
    "timestamp         = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir           = f\"runs/simpl_{timestamp}\"\n",
    "ckpt_dir          = os.path.join(\"checkpoints\", f\"simpl_{timestamp}\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "best_ckpt_path    = os.path.join(ckpt_dir, \"best_model.pt\")\n",
    "periodic_tpl      = os.path.join(ckpt_dir, \"epoch_{:04d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe667e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "batch_size = 128\n",
    "lr = 5e-4\n",
    "epochs = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Log hyperparameters\n",
    "hparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': lr,\n",
    "    'epochs': epochs,\n",
    "    'model_type': 'SIMPL',\n",
    "    'd_model': 128,\n",
    "    'nhead': 8,\n",
    "    'num_layers_temporal': 4,\n",
    "    'num_layers_social': 2,\n",
    "    'polynomial_degree': 5,\n",
    "    'dropout': 0.2,\n",
    "    'weight_decay': 1e-4,\n",
    "}\n",
    "writer.add_text('hyperparameters', str(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c4275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "print(\"Loading data...\")\n",
    "full_data = np.load(train_input)['data']\n",
    "\n",
    "# Split into train and eval (7:3)\n",
    "num_samples = len(full_data)\n",
    "num_train = int(0.7 * num_samples)\n",
    "perm = np.random.permutation(num_samples)\n",
    "train_idx = perm[:num_train]\n",
    "eval_idx = perm[num_train:]\n",
    "\n",
    "train_data = full_data[train_idx]\n",
    "eval_data = full_data[eval_idx]\n",
    "\n",
    "train_ds = TrajectoryDataset(data=train_data)\n",
    "eval_ds = TrajectoryDataset(data=eval_data)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(eval_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_ds = TrajectoryDataset(test_input, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Log dataset information\n",
    "writer.add_text('dataset_info', f\"Train samples: {len(train_ds)}, Eval samples: {len(eval_ds)}, Test samples: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d926097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create SIMPL model\n",
    "print(f\"Creating model on device: {device}\")\n",
    "model = SIMPLModel(\n",
    "    feature_dim=6,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers_temporal=2,\n",
    "    num_layers_social=2,\n",
    "    dim_feedforward=256,\n",
    "    T_past=50,\n",
    "    T_future=60,\n",
    "    polynomial_degree=5,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Log model architecture and parameters\n",
    "writer.add_text('model_architecture', str(model))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "writer.add_text('model_params', f\"Total trainable parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db2b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3b6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Resuming from epoch 100 (val_loss=271829.493948)\n"
     ]
    }
   ],
   "source": [
    "start_epoch   = 1\n",
    "best_val_loss = float('inf')\n",
    "global_step   = 0\n",
    "if os.path.exists(best_ckpt_path):\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    start_epoch   = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt.get('val_loss', best_val_loss)\n",
    "    global_step   = ckpt.get('global_step', 0)\n",
    "    print(f\"▶ Resuming from epoch {ckpt['epoch']} (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Log the training loop start\n",
    "writer.add_text('training_info', f\"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 289.442177, Time: 14.75s\n",
      "Epoch 101/1000 | Train Loss: 356050.780979 | Val ADE: 373.6232 | Val FDE: 373.2393 | Time: 16.49s\n",
      "Training - Loss: 283.786454, Time: 14.05s\n",
      "Epoch 102/1000 | Train Loss: 327957.332135 | Val ADE: 303.4437 | Val FDE: 304.8571 | Time: 15.80s\n",
      "Training - Loss: 285.559433, Time: 14.06s\n",
      "Epoch 103/1000 | Train Loss: 349915.540417 | Val ADE: 340.9776 | Val FDE: 336.2518 | Time: 15.81s\n",
      "Training - Loss: 283.156480, Time: 14.12s\n",
      "Epoch 104/1000 | Train Loss: 382429.337250 | Val ADE: 458.7293 | Val FDE: 454.8622 | Time: 15.88s\n",
      "Training - Loss: 282.769001, Time: 14.16s\n",
      "Epoch 105/1000 | Train Loss: 314905.156885 | Val ADE: 306.8951 | Val FDE: 307.9825 | Time: 15.94s\n",
      "Training - Loss: 280.390275, Time: 14.15s\n",
      "Epoch 106/1000 | Train Loss: 310226.962438 | Val ADE: 310.0016 | Val FDE: 311.0648 | Time: 15.92s\n",
      "Training - Loss: 278.970572, Time: 14.12s\n",
      "Epoch 107/1000 | Train Loss: 361528.672937 | Val ADE: 419.5420 | Val FDE: 416.0547 | Time: 15.89s\n",
      "Training - Loss: 279.754475, Time: 14.07s\n",
      "Epoch 108/1000 | Train Loss: 354782.077250 | Val ADE: 454.2345 | Val FDE: 449.6574 | Time: 15.86s\n",
      "Training - Loss: 276.122799, Time: 14.21s\n",
      "Epoch 109/1000 | Train Loss: 302792.408000 | Val ADE: 307.9259 | Val FDE: 304.4841 | Time: 15.96s\n",
      "Training - Loss: 273.338572, Time: 14.19s\n",
      "Epoch 110/1000 | Train Loss: 281904.597354 | Val ADE: 274.3603 | Val FDE: 275.9475 | Time: 15.95s\n",
      "Training - Loss: 268.393584, Time: 14.30s\n",
      "Epoch 111/1000 | Train Loss: 272563.730156 | Val ADE: 277.8079 | Val FDE: 279.3900 | Time: 16.07s\n",
      "Training - Loss: 263.021706, Time: 14.32s\n",
      "Epoch 112/1000 | Train Loss: 292566.753333 | Val ADE: 319.8516 | Val FDE: 316.6408 | Time: 16.10s\n",
      "Training - Loss: 260.691004, Time: 14.18s\n",
      "Epoch 113/1000 | Train Loss: 265713.941646 | Val ADE: 334.5888 | Val FDE: 333.3530 | Time: 15.96s\n",
      "✅ Best model saved at epoch 113 (val loss: 265713.941646)\n",
      "Training - Loss: 261.676162, Time: 14.23s\n",
      "Epoch 114/1000 | Train Loss: 238994.869875 | Val ADE: 257.1174 | Val FDE: 256.9978 | Time: 15.99s\n",
      "✅ Best model saved at epoch 114 (val loss: 238994.869875)\n",
      "Training - Loss: 266.781296, Time: 14.13s\n",
      "Epoch 115/1000 | Train Loss: 259780.349708 | Val ADE: 295.8765 | Val FDE: 295.5662 | Time: 15.90s\n",
      "Training - Loss: 262.111741, Time: 14.16s\n",
      "Epoch 116/1000 | Train Loss: 270564.767854 | Val ADE: 320.9335 | Val FDE: 319.5581 | Time: 15.94s\n",
      "Training - Loss: 254.148692, Time: 14.46s\n",
      "Epoch 117/1000 | Train Loss: 262981.109604 | Val ADE: 374.6834 | Val FDE: 373.0722 | Time: 16.24s\n",
      "Training - Loss: 256.360639, Time: 14.19s\n",
      "Epoch 118/1000 | Train Loss: 245191.227000 | Val ADE: 290.6639 | Val FDE: 292.3899 | Time: 15.96s\n",
      "Training - Loss: 252.403381, Time: 14.13s\n",
      "Epoch 119/1000 | Train Loss: 260521.193333 | Val ADE: 348.1340 | Val FDE: 353.7463 | Time: 15.91s\n",
      "Training - Loss: 253.173596, Time: 14.20s\n",
      "Epoch 120/1000 | Train Loss: 235247.195188 | Val ADE: 267.3753 | Val FDE: 267.8929 | Time: 15.97s\n",
      "✅ Best model saved at epoch 120 (val loss: 235247.195188)\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0120.pt\n",
      "Training - Loss: 251.114240, Time: 14.31s\n",
      "Epoch 121/1000 | Train Loss: 279503.871792 | Val ADE: 394.1201 | Val FDE: 391.0933 | Time: 16.09s\n",
      "Training - Loss: 256.384804, Time: 14.18s\n",
      "Epoch 122/1000 | Train Loss: 211501.825313 | Val ADE: 278.1958 | Val FDE: 275.0936 | Time: 15.96s\n",
      "✅ Best model saved at epoch 122 (val loss: 211501.825313)\n",
      "Training - Loss: 245.938848, Time: 14.23s\n",
      "Epoch 123/1000 | Train Loss: 204029.484969 | Val ADE: 235.6241 | Val FDE: 236.5742 | Time: 16.08s\n",
      "✅ Best model saved at epoch 123 (val loss: 204029.484969)\n",
      "Training - Loss: 243.900219, Time: 14.23s\n",
      "Epoch 124/1000 | Train Loss: 198479.893510 | Val ADE: 226.2928 | Val FDE: 229.3372 | Time: 16.02s\n",
      "✅ Best model saved at epoch 124 (val loss: 198479.893510)\n",
      "Training - Loss: 253.083461, Time: 14.21s\n",
      "Epoch 125/1000 | Train Loss: 196513.849187 | Val ADE: 262.2509 | Val FDE: 262.1512 | Time: 15.99s\n",
      "✅ Best model saved at epoch 125 (val loss: 196513.849187)\n",
      "Training - Loss: 241.580754, Time: 14.19s\n",
      "Epoch 126/1000 | Train Loss: 186188.681396 | Val ADE: 252.2542 | Val FDE: 251.2418 | Time: 15.97s\n",
      "✅ Best model saved at epoch 126 (val loss: 186188.681396)\n",
      "Training - Loss: 243.444039, Time: 14.45s\n",
      "Epoch 127/1000 | Train Loss: 187109.348771 | Val ADE: 240.1282 | Val FDE: 240.9652 | Time: 16.24s\n",
      "Training - Loss: 238.695502, Time: 14.26s\n",
      "Epoch 128/1000 | Train Loss: 179764.236521 | Val ADE: 254.4292 | Val FDE: 252.7892 | Time: 16.05s\n",
      "✅ Best model saved at epoch 128 (val loss: 179764.236521)\n",
      "Training - Loss: 241.051867, Time: 14.20s\n",
      "Epoch 129/1000 | Train Loss: 205083.602813 | Val ADE: 275.7223 | Val FDE: 278.8677 | Time: 15.98s\n",
      "Training - Loss: 236.364046, Time: 14.52s\n",
      "Epoch 130/1000 | Train Loss: 168506.285760 | Val ADE: 235.0295 | Val FDE: 236.3129 | Time: 16.30s\n",
      "✅ Best model saved at epoch 130 (val loss: 168506.285760)\n",
      "Training - Loss: 236.605069, Time: 14.22s\n",
      "Epoch 131/1000 | Train Loss: 170123.328271 | Val ADE: 235.1157 | Val FDE: 237.4439 | Time: 15.99s\n",
      "Training - Loss: 234.821869, Time: 14.21s\n",
      "Epoch 132/1000 | Train Loss: 169785.842021 | Val ADE: 221.4400 | Val FDE: 221.7888 | Time: 16.00s\n",
      "Training - Loss: 233.176241, Time: 14.38s\n",
      "Epoch 133/1000 | Train Loss: 176721.506083 | Val ADE: 248.4029 | Val FDE: 249.0723 | Time: 16.17s\n",
      "Training - Loss: 228.721414, Time: 14.40s\n",
      "Epoch 134/1000 | Train Loss: 183590.974271 | Val ADE: 279.7286 | Val FDE: 278.3473 | Time: 16.19s\n",
      "Training - Loss: 227.418540, Time: 14.29s\n",
      "Epoch 135/1000 | Train Loss: 168714.238917 | Val ADE: 252.1947 | Val FDE: 254.4309 | Time: 16.06s\n",
      "Training - Loss: 227.639157, Time: 14.40s\n",
      "Epoch 136/1000 | Train Loss: 164076.956229 | Val ADE: 216.5690 | Val FDE: 216.3949 | Time: 16.18s\n",
      "✅ Best model saved at epoch 136 (val loss: 164076.956229)\n",
      "Training - Loss: 222.409878, Time: 14.40s\n",
      "Epoch 137/1000 | Train Loss: 172830.576969 | Val ADE: 236.8693 | Val FDE: 240.7807 | Time: 16.19s\n",
      "Training - Loss: 226.032850, Time: 14.47s\n",
      "Epoch 138/1000 | Train Loss: 156305.782625 | Val ADE: 264.7796 | Val FDE: 264.9895 | Time: 16.27s\n",
      "✅ Best model saved at epoch 138 (val loss: 156305.782625)\n",
      "Training - Loss: 222.071416, Time: 14.26s\n",
      "Epoch 139/1000 | Train Loss: 169474.830375 | Val ADE: 262.2231 | Val FDE: 262.7433 | Time: 16.03s\n",
      "Training - Loss: 225.036961, Time: 14.18s\n",
      "Epoch 140/1000 | Train Loss: 166527.484583 | Val ADE: 234.3632 | Val FDE: 238.4532 | Time: 15.96s\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0140.pt\n",
      "Training - Loss: 223.001854, Time: 14.10s\n",
      "Epoch 141/1000 | Train Loss: 178249.394208 | Val ADE: 264.0402 | Val FDE: 262.6662 | Time: 15.86s\n",
      "Training - Loss: 219.777862, Time: 14.15s\n",
      "Epoch 142/1000 | Train Loss: 153872.098333 | Val ADE: 249.2813 | Val FDE: 249.1315 | Time: 15.93s\n",
      "✅ Best model saved at epoch 142 (val loss: 153872.098333)\n",
      "Training - Loss: 222.083614, Time: 14.15s\n",
      "Epoch 143/1000 | Train Loss: 138877.314521 | Val ADE: 207.9803 | Val FDE: 211.8661 | Time: 15.91s\n",
      "✅ Best model saved at epoch 143 (val loss: 138877.314521)\n",
      "Training - Loss: 217.253336, Time: 14.20s\n",
      "Epoch 144/1000 | Train Loss: 133805.619604 | Val ADE: 209.7158 | Val FDE: 209.0978 | Time: 15.99s\n",
      "✅ Best model saved at epoch 144 (val loss: 133805.619604)\n",
      "Training - Loss: 217.974814, Time: 14.16s\n",
      "Epoch 145/1000 | Train Loss: 160114.686417 | Val ADE: 241.2472 | Val FDE: 240.3358 | Time: 15.94s\n",
      "Training - Loss: 214.874557, Time: 14.38s\n",
      "Epoch 146/1000 | Train Loss: 137476.395313 | Val ADE: 216.8613 | Val FDE: 218.0004 | Time: 16.14s\n",
      "Training - Loss: 216.838512, Time: 14.23s\n",
      "Epoch 147/1000 | Train Loss: 172496.341062 | Val ADE: 307.7038 | Val FDE: 307.9472 | Time: 16.00s\n",
      "Training - Loss: 213.812940, Time: 14.13s\n",
      "Epoch 148/1000 | Train Loss: 148846.542562 | Val ADE: 235.5426 | Val FDE: 239.4747 | Time: 15.90s\n",
      "Training - Loss: 214.519555, Time: 14.13s\n",
      "Epoch 149/1000 | Train Loss: 158908.268875 | Val ADE: 276.7712 | Val FDE: 276.3421 | Time: 15.90s\n",
      "Training - Loss: 219.031923, Time: 14.21s\n",
      "Epoch 150/1000 | Train Loss: 167995.116937 | Val ADE: 240.4855 | Val FDE: 241.6358 | Time: 15.98s\n",
      "Training - Loss: 216.413554, Time: 14.18s\n",
      "Epoch 151/1000 | Train Loss: 150491.779062 | Val ADE: 210.2250 | Val FDE: 214.8756 | Time: 15.97s\n",
      "Training - Loss: 216.349737, Time: 14.24s\n",
      "Epoch 152/1000 | Train Loss: 158858.632823 | Val ADE: 265.5858 | Val FDE: 264.1732 | Time: 16.03s\n",
      "Training - Loss: 210.691772, Time: 14.18s\n",
      "Epoch 153/1000 | Train Loss: 165560.904958 | Val ADE: 220.2516 | Val FDE: 223.3873 | Time: 15.94s\n",
      "Training - Loss: 209.673060, Time: 14.12s\n",
      "Epoch 154/1000 | Train Loss: 170936.505125 | Val ADE: 268.7042 | Val FDE: 268.4453 | Time: 15.88s\n",
      "Training - Loss: 208.878140, Time: 14.17s\n",
      "Epoch 155/1000 | Train Loss: 132312.246271 | Val ADE: 206.1716 | Val FDE: 208.9647 | Time: 15.95s\n",
      "✅ Best model saved at epoch 155 (val loss: 132312.246271)\n",
      "Training - Loss: 215.951983, Time: 14.21s\n",
      "Epoch 156/1000 | Train Loss: 155563.814000 | Val ADE: 287.8640 | Val FDE: 288.0912 | Time: 15.99s\n",
      "Training - Loss: 211.969998, Time: 14.37s\n",
      "Epoch 157/1000 | Train Loss: 140374.401979 | Val ADE: 206.7541 | Val FDE: 208.8791 | Time: 16.14s\n",
      "Training - Loss: 211.164778, Time: 14.17s\n",
      "Epoch 158/1000 | Train Loss: 135259.273323 | Val ADE: 187.9733 | Val FDE: 190.6724 | Time: 15.94s\n",
      "Training - Loss: 208.759517, Time: 14.23s\n",
      "Epoch 159/1000 | Train Loss: 147481.248208 | Val ADE: 229.6145 | Val FDE: 229.2678 | Time: 16.00s\n",
      "Training - Loss: 208.278175, Time: 14.20s\n",
      "Epoch 160/1000 | Train Loss: 143093.299625 | Val ADE: 208.3627 | Val FDE: 211.0678 | Time: 15.97s\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0160.pt\n",
      "Training - Loss: 211.678566, Time: 14.17s\n",
      "Epoch 161/1000 | Train Loss: 151305.169042 | Val ADE: 248.0145 | Val FDE: 247.9958 | Time: 15.95s\n",
      "Training - Loss: 204.801357, Time: 14.19s\n",
      "Epoch 162/1000 | Train Loss: 143194.277729 | Val ADE: 229.0538 | Val FDE: 229.9478 | Time: 15.95s\n",
      "Training - Loss: 208.362616, Time: 14.17s\n",
      "Epoch 163/1000 | Train Loss: 146540.214417 | Val ADE: 244.8291 | Val FDE: 243.8789 | Time: 15.96s\n",
      "Training - Loss: 208.272366, Time: 14.25s\n",
      "Epoch 164/1000 | Train Loss: 140282.360125 | Val ADE: 225.2465 | Val FDE: 223.8367 | Time: 16.04s\n",
      "Training - Loss: 212.030708, Time: 14.25s\n",
      "Epoch 165/1000 | Train Loss: 155215.037271 | Val ADE: 210.9295 | Val FDE: 212.9795 | Time: 16.03s\n",
      "Training - Loss: 208.697984, Time: 14.31s\n",
      "Epoch 166/1000 | Train Loss: 149525.784187 | Val ADE: 232.5996 | Val FDE: 235.6871 | Time: 16.12s\n",
      "Training - Loss: 208.007114, Time: 14.20s\n",
      "Epoch 167/1000 | Train Loss: 142871.768292 | Val ADE: 268.4076 | Val FDE: 270.0444 | Time: 15.98s\n",
      "Training - Loss: 207.017316, Time: 14.25s\n",
      "Epoch 168/1000 | Train Loss: 122048.658437 | Val ADE: 188.9913 | Val FDE: 192.0507 | Time: 16.04s\n",
      "✅ Best model saved at epoch 168 (val loss: 122048.658437)\n",
      "Training - Loss: 203.083664, Time: 14.25s\n",
      "Epoch 169/1000 | Train Loss: 144808.147125 | Val ADE: 233.4612 | Val FDE: 234.2911 | Time: 16.03s\n",
      "Training - Loss: 209.630482, Time: 14.25s\n",
      "Epoch 170/1000 | Train Loss: 132274.068146 | Val ADE: 202.7026 | Val FDE: 204.7232 | Time: 16.03s\n",
      "Training - Loss: 204.823170, Time: 14.26s\n",
      "Epoch 171/1000 | Train Loss: 129024.929125 | Val ADE: 183.0419 | Val FDE: 184.0620 | Time: 16.06s\n",
      "Training - Loss: 206.730268, Time: 14.18s\n",
      "Epoch 172/1000 | Train Loss: 153110.433344 | Val ADE: 187.4493 | Val FDE: 189.8906 | Time: 15.97s\n",
      "Training - Loss: 201.418125, Time: 14.35s\n",
      "Epoch 173/1000 | Train Loss: 134335.054365 | Val ADE: 183.3174 | Val FDE: 182.8462 | Time: 16.14s\n",
      "Training - Loss: 201.566387, Time: 14.40s\n",
      "Epoch 174/1000 | Train Loss: 152891.425187 | Val ADE: 229.0425 | Val FDE: 228.8643 | Time: 16.20s\n",
      "Training - Loss: 203.516947, Time: 14.30s\n",
      "Epoch 175/1000 | Train Loss: 134652.664146 | Val ADE: 241.2963 | Val FDE: 241.1422 | Time: 16.09s\n",
      "Training - Loss: 200.484748, Time: 14.27s\n",
      "Epoch 176/1000 | Train Loss: 186581.484500 | Val ADE: 331.4080 | Val FDE: 331.5596 | Time: 16.07s\n",
      "Training - Loss: 204.567851, Time: 14.46s\n",
      "Epoch 177/1000 | Train Loss: 137941.021490 | Val ADE: 195.0036 | Val FDE: 196.3631 | Time: 16.26s\n",
      "Training - Loss: 203.356015, Time: 14.34s\n",
      "Epoch 178/1000 | Train Loss: 134720.198979 | Val ADE: 179.6571 | Val FDE: 181.5801 | Time: 16.12s\n",
      "Training - Loss: 198.638893, Time: 14.30s\n",
      "Epoch 179/1000 | Train Loss: 147838.153417 | Val ADE: 203.5387 | Val FDE: 204.2346 | Time: 16.09s\n",
      "Training - Loss: 197.202742, Time: 14.27s\n",
      "Epoch 180/1000 | Train Loss: 139256.203854 | Val ADE: 220.7178 | Val FDE: 221.8320 | Time: 16.08s\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0180.pt\n",
      "Training - Loss: 202.347750, Time: 14.31s\n",
      "Epoch 181/1000 | Train Loss: 131697.864917 | Val ADE: 221.9765 | Val FDE: 221.1002 | Time: 16.11s\n",
      "Training - Loss: 201.775526, Time: 14.32s\n",
      "Epoch 182/1000 | Train Loss: 133213.709396 | Val ADE: 184.8190 | Val FDE: 188.7894 | Time: 16.12s\n",
      "Training - Loss: 200.020273, Time: 14.28s\n",
      "Epoch 183/1000 | Train Loss: 160459.574000 | Val ADE: 243.8763 | Val FDE: 244.7303 | Time: 16.08s\n",
      "Training - Loss: 200.095327, Time: 14.36s\n",
      "Epoch 184/1000 | Train Loss: 152721.803250 | Val ADE: 236.5305 | Val FDE: 237.0477 | Time: 16.21s\n",
      "Training - Loss: 195.869426, Time: 14.27s\n",
      "Epoch 185/1000 | Train Loss: 137702.517458 | Val ADE: 188.9266 | Val FDE: 190.8237 | Time: 16.07s\n",
      "Training - Loss: 202.063142, Time: 14.46s\n",
      "Epoch 186/1000 | Train Loss: 119643.657667 | Val ADE: 164.6764 | Val FDE: 168.0172 | Time: 16.26s\n",
      "✅ Best model saved at epoch 186 (val loss: 119643.657667)\n",
      "Training - Loss: 202.131984, Time: 14.66s\n",
      "Epoch 187/1000 | Train Loss: 128884.668771 | Val ADE: 194.7405 | Val FDE: 197.4150 | Time: 16.54s\n",
      "Training - Loss: 199.693043, Time: 14.87s\n",
      "Epoch 188/1000 | Train Loss: 136986.088427 | Val ADE: 199.6275 | Val FDE: 203.7637 | Time: 16.66s\n",
      "Training - Loss: 201.311699, Time: 14.65s\n",
      "Epoch 189/1000 | Train Loss: 134398.596219 | Val ADE: 203.1618 | Val FDE: 206.2647 | Time: 16.46s\n",
      "Training - Loss: 197.770895, Time: 14.59s\n",
      "Epoch 190/1000 | Train Loss: 147869.630146 | Val ADE: 244.4119 | Val FDE: 246.9284 | Time: 16.40s\n",
      "Training - Loss: 197.206983, Time: 14.49s\n",
      "Epoch 191/1000 | Train Loss: 190087.719667 | Val ADE: 259.0732 | Val FDE: 260.5173 | Time: 16.28s\n",
      "Training - Loss: 197.899903, Time: 14.34s\n",
      "Epoch 192/1000 | Train Loss: 131246.028281 | Val ADE: 195.5688 | Val FDE: 197.7133 | Time: 16.13s\n",
      "Training - Loss: 194.043590, Time: 14.23s\n",
      "Epoch 193/1000 | Train Loss: 128129.159146 | Val ADE: 205.2047 | Val FDE: 207.7396 | Time: 16.03s\n",
      "Training - Loss: 196.429000, Time: 14.26s\n",
      "Epoch 194/1000 | Train Loss: 142092.069187 | Val ADE: 254.0886 | Val FDE: 254.7907 | Time: 16.03s\n",
      "Training - Loss: 195.484408, Time: 14.24s\n",
      "Epoch 195/1000 | Train Loss: 135751.753083 | Val ADE: 186.1176 | Val FDE: 188.5123 | Time: 16.04s\n",
      "Training - Loss: 193.202984, Time: 14.38s\n",
      "Epoch 196/1000 | Train Loss: 139805.959146 | Val ADE: 223.7843 | Val FDE: 225.2330 | Time: 16.18s\n",
      "Training - Loss: 197.792159, Time: 14.30s\n",
      "Epoch 197/1000 | Train Loss: 132081.968250 | Val ADE: 207.9114 | Val FDE: 210.8163 | Time: 16.10s\n",
      "Training - Loss: 193.027800, Time: 14.23s\n",
      "Epoch 198/1000 | Train Loss: 137799.052146 | Val ADE: 183.6375 | Val FDE: 185.1322 | Time: 16.06s\n",
      "Training - Loss: 193.348005, Time: 14.22s\n",
      "Epoch 199/1000 | Train Loss: 133350.556042 | Val ADE: 221.0617 | Val FDE: 222.4951 | Time: 16.03s\n",
      "Training - Loss: 196.879048, Time: 14.47s\n",
      "Epoch 200/1000 | Train Loss: 134871.382167 | Val ADE: 211.5527 | Val FDE: 214.3455 | Time: 16.25s\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0200.pt\n",
      "Training - Loss: 198.078929, Time: 14.49s\n",
      "Epoch 201/1000 | Train Loss: 135055.400292 | Val ADE: 204.7613 | Val FDE: 205.1452 | Time: 16.28s\n",
      "Training - Loss: 194.012770, Time: 14.53s\n",
      "Epoch 202/1000 | Train Loss: 142361.831958 | Val ADE: 209.8208 | Val FDE: 210.7909 | Time: 16.33s\n",
      "Training - Loss: 198.275454, Time: 14.48s\n",
      "Epoch 203/1000 | Train Loss: 128883.579354 | Val ADE: 175.1691 | Val FDE: 176.2541 | Time: 16.27s\n",
      "Training - Loss: 194.914685, Time: 14.35s\n",
      "Epoch 204/1000 | Train Loss: 134680.095219 | Val ADE: 199.9830 | Val FDE: 201.6415 | Time: 16.17s\n",
      "Training - Loss: 187.960261, Time: 14.75s\n",
      "Epoch 205/1000 | Train Loss: 131013.105146 | Val ADE: 176.5816 | Val FDE: 178.3288 | Time: 16.57s\n",
      "Training - Loss: 195.733477, Time: 14.43s\n",
      "Epoch 206/1000 | Train Loss: 135321.267406 | Val ADE: 185.7937 | Val FDE: 187.2726 | Time: 16.24s\n",
      "Training - Loss: 194.615721, Time: 14.54s\n",
      "Epoch 207/1000 | Train Loss: 143478.393906 | Val ADE: 201.4449 | Val FDE: 203.7140 | Time: 16.32s\n",
      "Training - Loss: 190.482896, Time: 14.70s\n",
      "Epoch 208/1000 | Train Loss: 137644.615823 | Val ADE: 181.1200 | Val FDE: 181.7492 | Time: 16.48s\n",
      "Training - Loss: 190.235865, Time: 14.38s\n",
      "Epoch 209/1000 | Train Loss: 132559.536958 | Val ADE: 176.4538 | Val FDE: 179.4954 | Time: 16.17s\n",
      "Training - Loss: 191.779751, Time: 14.31s\n",
      "Epoch 210/1000 | Train Loss: 149243.759854 | Val ADE: 163.8752 | Val FDE: 167.0254 | Time: 16.12s\n",
      "Training - Loss: 196.648097, Time: 14.31s\n",
      "Epoch 211/1000 | Train Loss: 144023.287021 | Val ADE: 231.9297 | Val FDE: 232.2424 | Time: 16.13s\n",
      "Training - Loss: 193.751420, Time: 14.36s\n",
      "Epoch 212/1000 | Train Loss: 137022.295771 | Val ADE: 183.6355 | Val FDE: 184.5004 | Time: 16.20s\n",
      "Training - Loss: 188.879518, Time: 14.38s\n",
      "Epoch 213/1000 | Train Loss: 131450.454542 | Val ADE: 183.5138 | Val FDE: 186.6862 | Time: 16.16s\n",
      "Training - Loss: 190.349011, Time: 14.63s\n",
      "Epoch 214/1000 | Train Loss: 147407.396365 | Val ADE: 207.3308 | Val FDE: 209.4272 | Time: 16.43s\n",
      "Training - Loss: 190.056897, Time: 14.44s\n",
      "Epoch 215/1000 | Train Loss: 138666.818302 | Val ADE: 174.3187 | Val FDE: 178.0752 | Time: 16.25s\n",
      "Training - Loss: 192.019951, Time: 14.30s\n",
      "Epoch 216/1000 | Train Loss: 140333.199552 | Val ADE: 181.6903 | Val FDE: 185.3960 | Time: 16.10s\n",
      "Training - Loss: 192.659288, Time: 14.67s\n",
      "Epoch 217/1000 | Train Loss: 136501.993958 | Val ADE: 187.7263 | Val FDE: 190.7994 | Time: 16.52s\n",
      "Training - Loss: 188.906983, Time: 13.78s\n",
      "Epoch 218/1000 | Train Loss: 124301.407396 | Val ADE: 172.2032 | Val FDE: 174.6133 | Time: 15.40s\n",
      "Training - Loss: 190.867687, Time: 13.22s\n",
      "Epoch 219/1000 | Train Loss: 159768.738625 | Val ADE: 250.7900 | Val FDE: 252.8269 | Time: 15.03s\n",
      "Training - Loss: 191.303536, Time: 14.62s\n",
      "Epoch 220/1000 | Train Loss: 143303.222729 | Val ADE: 208.3649 | Val FDE: 210.6604 | Time: 16.53s\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0220.pt\n",
      "Training - Loss: 193.164552, Time: 14.26s\n",
      "Epoch 221/1000 | Train Loss: 137972.292917 | Val ADE: 181.6839 | Val FDE: 185.5413 | Time: 16.04s\n",
      "Training - Loss: 187.180772, Time: 14.27s\n",
      "Epoch 222/1000 | Train Loss: 157538.440812 | Val ADE: 205.9960 | Val FDE: 207.2359 | Time: 16.05s\n",
      "Training - Loss: 191.070269, Time: 14.25s\n",
      "Epoch 223/1000 | Train Loss: 138081.856271 | Val ADE: 172.2212 | Val FDE: 175.6078 | Time: 16.02s\n",
      "Training - Loss: 191.545542, Time: 14.47s\n",
      "Epoch 224/1000 | Train Loss: 132865.645917 | Val ADE: 226.0515 | Val FDE: 229.9264 | Time: 16.37s\n",
      "Training - Loss: 188.080338, Time: 14.76s\n",
      "Epoch 225/1000 | Train Loss: 137342.033208 | Val ADE: 179.1675 | Val FDE: 182.6147 | Time: 16.57s\n",
      "Training - Loss: 188.332534, Time: 14.53s\n",
      "Epoch 226/1000 | Train Loss: 125120.009385 | Val ADE: 178.3278 | Val FDE: 179.9201 | Time: 16.34s\n",
      "Training - Loss: 190.556302, Time: 14.63s\n",
      "Epoch 227/1000 | Train Loss: 151143.631375 | Val ADE: 198.4176 | Val FDE: 202.0516 | Time: 16.42s\n",
      "Training - Loss: 186.582358, Time: 14.56s\n",
      "Epoch 228/1000 | Train Loss: 132307.171792 | Val ADE: 202.5339 | Val FDE: 204.5990 | Time: 16.38s\n",
      "Training - Loss: 187.189289, Time: 14.51s\n",
      "Epoch 229/1000 | Train Loss: 155261.933104 | Val ADE: 221.1815 | Val FDE: 223.7396 | Time: 16.34s\n",
      "Training - Loss: 190.945591, Time: 14.60s\n",
      "Epoch 230/1000 | Train Loss: 130493.500167 | Val ADE: 194.8424 | Val FDE: 197.3628 | Time: 16.43s\n",
      "Training - Loss: 189.821316, Time: 14.53s\n",
      "Epoch 231/1000 | Train Loss: 123845.708917 | Val ADE: 160.6412 | Val FDE: 164.4545 | Time: 16.34s\n",
      "Training - Loss: 189.068209, Time: 14.48s\n",
      "Epoch 232/1000 | Train Loss: 132600.273167 | Val ADE: 184.9860 | Val FDE: 188.8313 | Time: 16.27s\n",
      "Training - Loss: 190.773893, Time: 14.52s\n",
      "Epoch 233/1000 | Train Loss: 136141.240583 | Val ADE: 157.8436 | Val FDE: 162.1523 | Time: 16.32s\n",
      "Training - Loss: 183.246379, Time: 14.43s\n",
      "Epoch 234/1000 | Train Loss: 142501.961500 | Val ADE: 243.5082 | Val FDE: 246.7383 | Time: 16.22s\n",
      "Training - Loss: 184.980533, Time: 14.23s\n",
      "Epoch 235/1000 | Train Loss: 149753.835781 | Val ADE: 202.7951 | Val FDE: 206.5540 | Time: 16.03s\n",
      "Training - Loss: 189.133456, Time: 14.41s\n",
      "Epoch 236/1000 | Train Loss: 142371.585375 | Val ADE: 199.2368 | Val FDE: 204.4709 | Time: 16.21s\n",
      "Training - Loss: 185.833626, Time: 14.26s\n",
      "Epoch 237/1000 | Train Loss: 150350.266906 | Val ADE: 178.1973 | Val FDE: 181.9996 | Time: 16.09s\n",
      "Training - Loss: 185.289074, Time: 14.37s\n",
      "Epoch 238/1000 | Train Loss: 133626.379281 | Val ADE: 176.5118 | Val FDE: 179.6344 | Time: 16.17s\n",
      "Training - Loss: 182.855882, Time: 14.45s\n",
      "Epoch 239/1000 | Train Loss: 142914.800344 | Val ADE: 184.8675 | Val FDE: 187.4040 | Time: 16.26s\n",
      "Training - Loss: 182.225173, Time: 14.38s\n",
      "Epoch 240/1000 | Train Loss: 149506.096396 | Val ADE: 190.0122 | Val FDE: 193.4108 | Time: 16.23s\n",
      "🧪 Checkpoint saved at checkpoints/simpl_ckpt_epoch_0240.pt\n",
      "Training - Loss: 182.311574, Time: 14.35s\n",
      "Epoch 241/1000 | Train Loss: 140523.960208 | Val ADE: 171.8057 | Val FDE: 174.7920 | Time: 16.16s\n",
      "Training - Loss: 183.624435, Time: 14.49s\n",
      "Epoch 242/1000 | Train Loss: 154711.365510 | Val ADE: 194.4912 | Val FDE: 196.1191 | Time: 16.31s\n",
      "Training - Loss: 182.496331, Time: 14.46s\n",
      "Epoch 243/1000 | Train Loss: 151319.355406 | Val ADE: 237.2034 | Val FDE: 238.4811 | Time: 16.26s\n",
      "Training - Loss: 188.906795, Time: 14.39s\n",
      "Epoch 244/1000 | Train Loss: 156836.588406 | Val ADE: 213.1366 | Val FDE: 215.0882 | Time: 16.19s\n",
      "Training - Loss: 185.711884, Time: 14.29s\n",
      "Epoch 245/1000 | Train Loss: 135730.436094 | Val ADE: 171.7867 | Val FDE: 174.9300 | Time: 16.05s\n",
      "Training - Loss: 182.804305, Time: 14.36s\n",
      "Epoch 246/1000 | Train Loss: 154290.996698 | Val ADE: 187.0922 | Val FDE: 189.6478 | Time: 16.19s\n",
      "Training - Loss: 184.636809, Time: 14.59s\n",
      "Epoch 247/1000 | Train Loss: 152455.902443 | Val ADE: 192.4853 | Val FDE: 194.3177 | Time: 16.43s\n",
      "Training - Loss: 187.701555, Time: 14.80s\n",
      "Epoch 248/1000 | Train Loss: 141155.517792 | Val ADE: 168.3963 | Val FDE: 170.1967 | Time: 16.67s\n",
      "Training - Loss: 181.586337, Time: 14.70s\n",
      "Epoch 249/1000 | Train Loss: 156376.093948 | Val ADE: 192.0517 | Val FDE: 193.8136 | Time: 16.57s\n",
      "Training - Loss: 187.130218, Time: 14.60s\n",
      "Epoch 250/1000 | Train Loss: 146031.142354 | Val ADE: 166.7381 | Val FDE: 167.3040 | Time: 16.45s\n",
      "Training - Loss: 179.253507, Time: 14.83s\n",
      "Epoch 251/1000 | Train Loss: 144945.942052 | Val ADE: 207.3727 | Val FDE: 210.2932 | Time: 16.67s\n",
      "Training - Loss: 180.685819, Time: 14.52s\n",
      "Epoch 252/1000 | Train Loss: 134351.122740 | Val ADE: 169.6839 | Val FDE: 171.1224 | Time: 16.28s\n",
      "Training - Loss: 186.333322, Time: 14.21s\n",
      "Epoch 253/1000 | Train Loss: 166679.541979 | Val ADE: 290.9843 | Val FDE: 291.3496 | Time: 16.01s\n",
      "Training - Loss: 184.411321, Time: 14.31s\n",
      "Epoch 254/1000 | Train Loss: 155137.631729 | Val ADE: 214.9186 | Val FDE: 216.7964 | Time: 16.12s\n",
      "Training - Loss: 183.845513, Time: 14.39s\n",
      "Epoch 255/1000 | Train Loss: 146177.531005 | Val ADE: 194.8387 | Val FDE: 198.2856 | Time: 16.21s\n",
      "Training - Loss: 182.104659, Time: 14.37s\n",
      "Epoch 256/1000 | Train Loss: 139482.643250 | Val ADE: 209.9162 | Val FDE: 212.4809 | Time: 16.20s\n",
      "Training - Loss: 181.912819, Time: 14.42s\n",
      "Epoch 257/1000 | Train Loss: 164415.422927 | Val ADE: 203.4886 | Val FDE: 204.8868 | Time: 16.24s\n",
      "Training - Loss: 180.813742, Time: 14.43s\n",
      "Epoch 258/1000 | Train Loss: 160522.035823 | Val ADE: 173.8119 | Val FDE: 177.6673 | Time: 16.22s\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "try:\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train for one epoch\n",
    "        model, global_step = train(\n",
    "            model, train_loader, optimizer, device, \n",
    "            num_epochs=1, lr_scheduler=lr_scheduler,\n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(\n",
    "            model, eval_loader, device, \n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "        val_loss = val_metrics['loss']\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "                f\"Train Loss: {val_metrics['loss']:.6f} | \"\n",
    "                f\"Val ADE: {val_metrics['ade']:.4f} | \"\n",
    "                f\"Val FDE: {val_metrics['fde']:.4f} | \"\n",
    "                f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'global_step': global_step,\n",
    "                'hparams': hparams\n",
    "            }, best_ckpt_path)\n",
    "            print(f\"✅ Best model saved at epoch {epoch} (val loss: {best_val_loss:.6f})\")\n",
    "            writer.add_text('checkpoints', f\"New best model at epoch {epoch} with val_loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint_file = f'checkpoints/simpl_ckpt_epoch_{epoch:04d}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'global_step': global_step,\n",
    "                'hparams': hparams\n",
    "            }, checkpoint_file)\n",
    "            print(f\"🧪 Checkpoint saved at {checkpoint_file}\")\n",
    "            writer.add_text('checkpoints', f\"Periodic checkpoint at epoch {epoch}\")\n",
    "        \n",
    "        # # Early stopping check - if no improvement for 100 epochs\n",
    "        # if epoch > start_epoch + 100 and val_loss > best_val_loss * 0.99:\n",
    "        #     print(f\"Early stopping triggered. No significant improvement for 100 epochs.\")\n",
    "        #     writer.add_text('training_info', f\"Early stopping at epoch {epoch}\")\n",
    "        #     break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")\n",
    "    writer.add_text('training_info', f\"Training interrupted at epoch {epoch}\")\n",
    "\n",
    "finally:\n",
    "    # Calculate total training time\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
    "    writer.add_text('training_info', f\"Training completed/interrupted after {total_training_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_ckpt_path, map_location=device,weights_only=False)['model_state_dict'])\n",
    "preds = predict(model, test_loader, device)\n",
    "np.savetxt(output_csv, preds.reshape(-1, 2), delimiter=',')\n",
    "print(f\"Predictions saved to {output_csv}\")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
