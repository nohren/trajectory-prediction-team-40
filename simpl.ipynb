{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca95b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a1a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # Training data with future\n",
    "        pasts, masks, futures = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        future = torch.stack(futures)\n",
    "        return past, mask, future\n",
    "    else:  # Test data without future\n",
    "        pasts, masks = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        return past, mask\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, input_path=None, data=None, T_past=50, T_future=60,\n",
    "                 is_test=False, pos_mean=None, pos_std=None, vel_mean=None, vel_std=None):\n",
    "        if data is not None:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            npz = np.load(input_path)\n",
    "            self.data = npz['data']\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.is_test = is_test\n",
    "        # Normalization stats\n",
    "        self.pos_mean = np.array(pos_mean, dtype=np.float32) if pos_mean is not None else None\n",
    "        self.pos_std  = np.array(pos_std,  dtype=np.float32) if pos_std  is not None else None\n",
    "        self.vel_mean = np.array(vel_mean, dtype=np.float32) if vel_mean is not None else None\n",
    "        self.vel_std  = np.array(vel_std,  dtype=np.float32) if vel_std  is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx].astype(np.float32)  # (num_agents, T, 6)\n",
    "        # Apply global normalization\n",
    "        if self.pos_mean is not None:\n",
    "            scene[..., :2] = (scene[..., :2] - self.pos_mean) / self.pos_std\n",
    "            scene[..., 2:4] = (scene[..., 2:4] - self.vel_mean) / self.vel_std\n",
    "        past = scene[:, :self.T_past, :]\n",
    "        mask = np.sum(np.abs(past[..., :2]), axis=(1, 2)) > 0\n",
    "        if not self.is_test and scene.shape[1] >= self.T_past + self.T_future:\n",
    "            future = scene[0, self.T_past:self.T_past + self.T_future, :2]\n",
    "            return (\n",
    "                torch.tensor(past, dtype=torch.float32),\n",
    "                torch.tensor(mask, dtype=torch.bool),\n",
    "                torch.tensor(future, dtype=torch.float32)\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(past, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "class AugmentedTrajectoryDataset(Dataset):\n",
    "    def __init__(self, input_path=None, data=None, T_past=50, T_future=60, is_test=False,\n",
    "                 augment_prob=0.7, noise_scale=0.03, rotation_max=20, flip_prob=0.4):\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        else:\n",
    "            npz = np.load(input_path)\n",
    "            self.data = npz['data']\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        self.augment_prob = augment_prob\n",
    "        self.noise_scale = noise_scale\n",
    "        self.rotation_max = rotation_max\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _apply_augmentations(self, past, future=None):\n",
    "        \"\"\"Apply augmentations to the trajectory data\"\"\"\n",
    "        # Skip augmentation during testing or with probability (1-augment_prob)\n",
    "        if self.is_test or np.random.random() > self.augment_prob:\n",
    "            return past, future\n",
    "        \n",
    "        # Create copies to avoid modifying original data\n",
    "        past_aug = past.copy()\n",
    "        future_aug = future.copy() if future is not None else None\n",
    "        \n",
    "        # 1. Random rotation\n",
    "        if np.random.random() < 0.7:  # 70% chance of rotation\n",
    "            angle = np.random.uniform(-self.rotation_max, self.rotation_max)\n",
    "            angle_rad = np.radians(angle)\n",
    "            rot_matrix = np.array([\n",
    "                [np.cos(angle_rad), -np.sin(angle_rad)],\n",
    "                [np.sin(angle_rad), np.cos(angle_rad)]\n",
    "            ])\n",
    "            \n",
    "            # Apply rotation to positions\n",
    "            for i in range(past_aug.shape[0]):\n",
    "                for t in range(past_aug.shape[1]):\n",
    "                    past_aug[i, t, :2] = np.dot(rot_matrix, past_aug[i, t, :2])\n",
    "                    if past_aug.shape[2] > 2:  # If we have velocity data\n",
    "                        past_aug[i, t, 2:4] = np.dot(rot_matrix, past_aug[i, t, 2:4])\n",
    "            \n",
    "            # Apply same rotation to future if it exists\n",
    "            if future_aug is not None:\n",
    "                for t in range(future_aug.shape[0]):\n",
    "                    future_aug[t, :2] = np.dot(rot_matrix, future_aug[t, :2])\n",
    "        \n",
    "        # 2. Add Gaussian noise to positions\n",
    "        if np.random.random() < 0.6:  # 60% chance of adding noise\n",
    "            noise = np.random.normal(0, self.noise_scale, past_aug[..., :2].shape)\n",
    "            past_aug[..., :2] += noise\n",
    "            \n",
    "            # Update velocities to match the noisy positions\n",
    "            if past_aug.shape[2] > 2:\n",
    "                for i in range(past_aug.shape[0]):\n",
    "                    for t in range(1, past_aug.shape[1]):\n",
    "                        past_aug[i, t, 2:4] = past_aug[i, t, :2] - past_aug[i, t-1, :2]\n",
    "        \n",
    "        # 3. Random horizontal flipping\n",
    "        if np.random.random() < self.flip_prob:\n",
    "            # Flip x-coordinates\n",
    "            past_aug[..., 0] = -past_aug[..., 0]\n",
    "            # Flip x-velocities\n",
    "            if past_aug.shape[2] > 2:\n",
    "                past_aug[..., 2] = -past_aug[..., 2]\n",
    "            \n",
    "            # Flip future trajectory if it exists\n",
    "            if future_aug is not None:\n",
    "                future_aug[..., 0] = -future_aug[..., 0]\n",
    "        \n",
    "        return past_aug, future_aug\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]  # (num_agents, T, 6)\n",
    "        past = scene[:, :self.T_past, :]\n",
    "        mask = np.sum(np.abs(past[..., :2]), axis=(1, 2)) > 0\n",
    "        \n",
    "        if not self.is_test and scene.shape[1] >= self.T_past + self.T_future:\n",
    "            future = scene[0, self.T_past:self.T_past + self.T_future, :2]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            past_aug, future_aug = self._apply_augmentations(past, future)\n",
    "            \n",
    "            return (\n",
    "                torch.tensor(past_aug, dtype=torch.float32),\n",
    "                torch.tensor(mask, dtype=torch.bool),\n",
    "                torch.tensor(future_aug, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        # For test data, no augmentation\n",
    "        if self.is_test:\n",
    "            return (\n",
    "                torch.tensor(past, dtype=torch.float32),\n",
    "                torch.tensor(mask, dtype=torch.bool)\n",
    "            )\n",
    "            \n",
    "        return (\n",
    "            torch.tensor(past, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.bool)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a741e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernsteinLayer(nn.Module):\n",
    "    def __init__(self, n_degree, T=60):\n",
    "        super().__init__()\n",
    "        self.n_degree = n_degree\n",
    "        self.T = T\n",
    "        # Generate Bernstein basis functions\n",
    "        time_points = torch.linspace(0, 1, T)\n",
    "        bern = torch.zeros(n_degree + 1, T)\n",
    "        for i in range(n_degree + 1):\n",
    "            bern[i] = math.comb(n_degree, i) * (time_points ** i) * ((1 - time_points) ** (n_degree - i))\n",
    "        self.register_buffer('bernstein_values', bern)\n",
    "        # Calculate pseudoinverse for control-point regression\n",
    "        self.register_buffer('pinv', torch.pinverse(bern.T))\n",
    "        \n",
    "        # Log the shapes for debugging\n",
    "        print(f\"Bernstein basis shape: {bern.shape}, Pinv shape: {self.pinv.shape}\")\n",
    "\n",
    "    def forward(self, control_points):\n",
    "        \"\"\"Convert control points to trajectory using Bernstein polynomials\"\"\"\n",
    "        # control_points: (B, n+1, 2)\n",
    "        B, n_cp, D = control_points.shape\n",
    "        if n_cp != self.n_degree + 1:\n",
    "            raise ValueError(f\"Expected {self.n_degree+1} control points, got {n_cp}\")\n",
    "            \n",
    "        cp = control_points.transpose(1, 2)  # (B,2,n+1)\n",
    "        traj = torch.matmul(cp, self.bernstein_values)  # (B,2,T)\n",
    "        return traj.transpose(1, 2)  # (B,T,2)\n",
    "\n",
    "    def inverse(self, trajectories):\n",
    "        \"\"\"Convert trajectory to control points using pseudoinverse\"\"\"\n",
    "        # trajectories: (B,T,2) -> (B,n+1,2)\n",
    "        B, T, D = trajectories.shape\n",
    "        \n",
    "        # Verify input dimensions match expected dimensions\n",
    "        if T != self.T:\n",
    "            raise ValueError(f\"Expected trajectories with length {self.T}, but got {T}\")\n",
    "            \n",
    "        # Use einsum for efficient batch matrix multiplication\n",
    "        try:\n",
    "            # pinv: (T,n+1)\n",
    "             return torch.einsum('nt,btd->bnd', self.pinv, trajectories)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in Bernstein inverse: Pinv shape={self.pinv.shape}, Traj shape={trajectories.shape}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0075f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pos_enc = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pos_enc', pos_enc.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pos_enc[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8f59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rpe_processor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, source, target, rpe=None):\n",
    "        B, Ns, D = source.shape\n",
    "        _, Nt, _ = target.shape\n",
    "        query = self.query_proj(target)\n",
    "        key = self.key_proj(source)\n",
    "        value = self.value_proj(source)\n",
    "        if rpe is not None:\n",
    "            # rpe: (B, N_target, N_source, D)\n",
    "            proc = self.rpe_processor(rpe)                         # (B, N_target, N_source, D)\n",
    "            key_expanded = key.unsqueeze(1)                        # (B, 1, N_source, D)\n",
    "            enhanced = key_expanded + proc                         # broadcasts to (B, N_target, N_source, D)\n",
    "            outputs = []\n",
    "            for i in range(Nt):\n",
    "                q = query[:, i:i+1]                                # (B, 1, D)\n",
    "                k = enhanced[:, i, :, :]                           # (B, N_source, D)\n",
    "                v = value                                          # (B, N_source, D)\n",
    "                out, _ = self.multihead_attn(q, k, v)              # now all 3-D\n",
    "                outputs.append(out)\n",
    "            attn_output = torch.cat(outputs, dim=1)                # (B, N_target, D)\n",
    "        else:\n",
    "            attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        tgt2 = self.norm1(target + self.dropout(attn_output))\n",
    "        ff_out = self.ff(tgt2)\n",
    "        return self.norm2(tgt2 + self.dropout(ff_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0a6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIMPLModel(nn.Module):\n",
    "    def __init__(self, feature_dim=6, d_model=128, nhead=8,\n",
    "                 num_layers_temporal=2, num_layers_social=2,\n",
    "                 dim_feedforward=256, T_past=50, T_future=60,\n",
    "                 polynomial_degree=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "\n",
    "        # Feature normalization layers\n",
    "        self.position_norm = nn.LayerNorm([2])\n",
    "        self.velocity_norm = nn.LayerNorm([2])\n",
    "        self.other_norm = nn.LayerNorm([feature_dim-4]) if feature_dim>4 else None\n",
    "\n",
    "        # Input embedding\n",
    "        self.input_embed = nn.Linear(feature_dim, d_model)\n",
    "        self.time_pos_enc = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        # Temporal encoders\n",
    "        self.temporal_encoders = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers_temporal)\n",
    "        ])\n",
    "\n",
    "        # Relative position embedding\n",
    "        self.rpe_generator = nn.Sequential(\n",
    "            nn.Linear(feature_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "        # Social attention layers\n",
    "        self.social_encoders = nn.ModuleList([\n",
    "            nn.MultiheadAttention(\n",
    "                embed_dim=d_model,\n",
    "                num_heads=nhead,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers_social)\n",
    "        ])\n",
    "        \n",
    "        # Social layer norms\n",
    "        self.social_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model)\n",
    "            for _ in range(num_layers_social)\n",
    "        ])\n",
    "\n",
    "        # Control point predictor (with proper initialization)\n",
    "        self.control_point_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.LayerNorm(dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, dim_feedforward),\n",
    "            nn.LayerNorm(dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, 2*(polynomial_degree+1))\n",
    "        )\n",
    "\n",
    "        # Create Bernstein layer with matching dimension\n",
    "        self.bernstein_layer = BernsteinLayer(polynomial_degree, T_future)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.temporal_skip = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Initialize weights with improved method\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights with Xavier/Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Use Kaiming/He initialization for ReLU layers\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def compute_relative_position_embedding(self, past, mask):\n",
    "        \"\"\"Generate relative position embedding between agents\"\"\"\n",
    "        B, N, T, F = past.shape\n",
    "        # Get last position of each agent\n",
    "        last = past[:, :, -1, :]\n",
    "        # Expand ego vehicle position\n",
    "        ego = last[:, 0:1, :].expand(-1, N, -1)\n",
    "        # Calculate relative positions and apply mask\n",
    "        rpe_feats = (ego - last) * mask.unsqueeze(-1).float()\n",
    "        return self.rpe_generator(rpe_feats)\n",
    "\n",
    "    def forward(self, past, agent_mask, return_cps=False):\n",
    "        \"\"\"Forward pass of the model\"\"\"\n",
    "        B, N, T, F = past.shape\n",
    "        \n",
    "        # Ensure ego vehicle is always included in mask\n",
    "        agent_mask = agent_mask.clone()\n",
    "        agent_mask[:, 0] = True\n",
    "        \n",
    "        # Normalize features\n",
    "        x = past.reshape(B*N*T, F)\n",
    "        pos = x[:, :2]; pos = self.position_norm(pos)\n",
    "        vel = x[:, 2:4]; vel = self.velocity_norm(vel)\n",
    "        if F > 4 and self.other_norm is not None:\n",
    "            oth = x[:, 4:]; oth = self.other_norm(oth)\n",
    "            x_norm = torch.cat([pos, vel, oth], dim=1)\n",
    "        else:\n",
    "            x_norm = torch.cat([pos, vel], dim=1)\n",
    "        \n",
    "        # Reshape for temporal processing\n",
    "        x = x_norm.reshape(B*N, T, F)\n",
    "        \n",
    "        # Apply input embedding\n",
    "        x = self.input_embed(x)\n",
    "        \n",
    "        # Apply layer normalization to stabilize training\n",
    "        x = x / (x.norm(dim=-1, keepdim=True) + 1e-6) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.time_pos_enc(x)\n",
    "        \n",
    "        # Store features for skip connection\n",
    "        temp_feats = x.clone()\n",
    "        \n",
    "        # Apply temporal encoders\n",
    "        for layer in self.temporal_encoders:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Extract feature from last timestep\n",
    "        x = x[:, -1, :].reshape(B, N, self.d_model)\n",
    "        \n",
    "        # Compute relative position embedding\n",
    "        rpe = self.compute_relative_position_embedding(past, agent_mask)\n",
    "        \n",
    "        # Extract ego features and prepare for social attention\n",
    "        ego_feats = x[:, 0:1, :]  # (B, 1, D)\n",
    "        \n",
    "        # Apply social attention layers\n",
    "        for i, (attn, norm) in enumerate(zip(self.social_encoders, self.social_norms)):\n",
    "            # Create key padding mask for attention\n",
    "            key_padding_mask = ~agent_mask  # (B, N)\n",
    "            \n",
    "            # Apply attention\n",
    "            attn_output, _ = attn(\n",
    "                query=ego_feats,  # (B, 1, D)\n",
    "                key=x,            # (B, N, D)\n",
    "                value=x,          # (B, N, D)\n",
    "                key_padding_mask=key_padding_mask\n",
    "            )\n",
    "            \n",
    "            # Apply normalization and residual connection\n",
    "            ego_feats = norm(ego_feats + attn_output)\n",
    "        \n",
    "        # Extract final ego embedding\n",
    "        ego_embed = ego_feats.squeeze(1)  # (B, D)\n",
    "        \n",
    "        # Apply skip connection from temporal features\n",
    "        temp_skip = self.temporal_skip(temp_feats.reshape(B*N, T, self.d_model)[:, -1, :])\n",
    "        temp_skip = temp_skip.reshape(B, N, self.d_model)[:, 0, :]\n",
    "        ego_embed = ego_embed + temp_skip\n",
    "        \n",
    "        # Predict control points\n",
    "        cps_flat = self.control_point_predictor(ego_embed)\n",
    "        cps = cps_flat.reshape(B, self.polynomial_degree+1, 2)\n",
    "        \n",
    "        # Convert control points to trajectory\n",
    "        traj = self.bernstein_layer(cps)\n",
    "        \n",
    "        if return_cps:\n",
    "            return traj, cps\n",
    "        return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8a3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, base_lr=3e-5):\n",
    "    backbone_params, predictor_params, embedding_params, bernstein_params = [], [], [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'control_point_predictor' in name:\n",
    "            predictor_params.append(param)\n",
    "        elif 'bernstein_layer' in name:\n",
    "            bernstein_params.append(param)\n",
    "        elif 'input_embed' in name:\n",
    "            embedding_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "    return torch.optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': base_lr,    'weight_decay':1e-5},\n",
    "        {'params': predictor_params,'lr': base_lr*1.5,'weight_decay':2e-5},\n",
    "        {'params': embedding_params,'lr': base_lr*1.2,'weight_decay':1e-5},\n",
    "        {'params': bernstein_params,'lr': base_lr*0.5,'weight_decay':1e-6},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a2d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device, epoch,\n",
    "          lr_scheduler=None, writer=None, global_step=0, lambda_cp=0.1):\n",
    "    model.train()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    scaler = GradScaler()\n",
    "    total_loss = 0.0\n",
    "    total_traj_loss = 0.0\n",
    "    total_cp_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    # Implement curriculum learning for trajectory horizon\n",
    "    if epoch <= 10:\n",
    "        H = 10\n",
    "    elif epoch <= 20:\n",
    "        H = 20\n",
    "    elif epoch <= 40:\n",
    "        H = 40\n",
    "    else:\n",
    "        H = model.T_future\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        past, mask, future = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Skip problematic batches\n",
    "        if torch.isnan(past).any() or torch.isnan(future).any():\n",
    "            print(f\"Warning: NaN values detected in batch {batch_idx}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            with autocast(device.type):\n",
    "                # Forward pass with control points\n",
    "                pred, cps_pred = model(past, mask, return_cps=True)\n",
    "                \n",
    "                # Trajectory loss over current horizon H\n",
    "                loss_traj = criterion(pred[:, :H, :], future[:, :H, :])\n",
    "                \n",
    "                # Check if we have full future trajectory for CP loss\n",
    "                if future.size(1) == model.T_future:\n",
    "                    # Calculate control points for ground truth trajectory\n",
    "                    cp_true = model.bernstein_layer.inverse(future)\n",
    "                    loss_cp = F.mse_loss(cps_pred, cp_true)\n",
    "                else:\n",
    "                    loss_cp = torch.tensor(0.0, device=pred.device)\n",
    "                \n",
    "                # Combined loss with weighting\n",
    "                loss = loss_traj + lambda_cp * loss_cp\n",
    "                \n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Warning: NaN loss detected in batch {batch_idx}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Gradient scaling for mixed precision\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Step optimizer with scaling\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update learning rate if scheduler provided\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            # Log metrics\n",
    "            if writer is not None and batch_idx % 20 == 0:\n",
    "                writer.add_scalar('train/batch_loss', loss.item(), global_step)\n",
    "                writer.add_scalar('train/batch_traj_loss', loss_traj.item(), global_step)\n",
    "                if loss_cp.item() > 0:\n",
    "                    writer.add_scalar('train/batch_cp_loss', loss_cp.item(), global_step)\n",
    "                writer.add_scalar('train/batch_horizon', H, global_step)\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                writer.add_scalar('train/learning_rate', lr, global_step)\n",
    "                \n",
    "            # Update counters\n",
    "            global_step += 1\n",
    "            total_loss += loss.item()\n",
    "            total_traj_loss += loss_traj.item()\n",
    "            total_cp_loss += loss_cp.item() if loss_cp.item() > 0 else 0\n",
    "            batch_count += 1\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            # Print tensor shapes for debugging\n",
    "            print(f\"past: {past.shape}, mask: {mask.shape}, future: {future.shape}\")\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                # Try to recover from OOM error\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                # For other errors, we might want to raise to debug\n",
    "                raise e\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_loss = total_loss / max(batch_count, 1)\n",
    "    avg_traj_loss = total_traj_loss / max(batch_count, 1)\n",
    "    avg_cp_loss = total_cp_loss / max(batch_count, 1)\n",
    "    \n",
    "    # Log epoch metrics\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('train/epoch_loss', avg_loss, epoch)\n",
    "        writer.add_scalar('train/epoch_traj_loss', avg_traj_loss, epoch)\n",
    "        writer.add_scalar('train/epoch_cp_loss', avg_cp_loss, epoch)\n",
    "    \n",
    "    return model, global_step, avg_loss\n",
    "\n",
    "# Evaluation function with tensorboard logging\n",
    "def evaluate(model, val_loader, device, writer=None, global_step=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    mse_criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    all_errors = []\n",
    "    all_ades = []\n",
    "    all_fdes = []\n",
    "    \n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            past, mask, future = [x.to(device) for x in batch]\n",
    "            \n",
    "            # No need for autocast during evaluation\n",
    "            pred = model(past, mask)\n",
    "            \n",
    "            # Calculate MSE loss per time step and sample\n",
    "            mse = mse_criterion(pred, future)  # (B, T, 2)\n",
    "            \n",
    "            # Calculate ADE (Average Displacement Error)\n",
    "            ade = torch.sqrt(mse.sum(dim=2)).mean(dim=1)  # (B,)\n",
    "            all_ades.extend(ade.cpu().numpy())\n",
    "            \n",
    "            # Calculate FDE (Final Displacement Error)\n",
    "            fde = torch.sqrt(mse[:, -1].sum(dim=1))  # (B,)\n",
    "            all_fdes.extend(fde.cpu().numpy())\n",
    "            \n",
    "            # Store errors for reporting\n",
    "            for i in range(len(ade)):\n",
    "                all_errors.append({\n",
    "                    'ade': ade[i].item(),\n",
    "                    'fde': fde[i].item()\n",
    "                })\n",
    "            \n",
    "            # Accumulate MSE loss\n",
    "            batch_loss = mse.mean()\n",
    "            total_loss += batch_loss.item() * past.size(0)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    avg_ade = np.mean(all_ades)\n",
    "    avg_fde = np.mean(all_fdes)\n",
    "    \n",
    "    # Calculate additional metrics - percentiles\n",
    "    ade_50 = np.percentile(all_ades, 50)  # median\n",
    "    ade_90 = np.percentile(all_ades, 90)  # 90th percentile\n",
    "    fde_50 = np.percentile(all_fdes, 50)  # median\n",
    "    fde_90 = np.percentile(all_fdes, 90)  # 90th percentile\n",
    "    \n",
    "    eval_time = time.time() - eval_start_time\n",
    "    \n",
    "    # Log metrics to tensorboard\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar('val/loss', avg_loss, global_step)\n",
    "        writer.add_scalar('val/ade_mean', avg_ade, global_step)\n",
    "        writer.add_scalar('val/fde_mean', avg_fde, global_step)\n",
    "        writer.add_scalar('val/ade_50', ade_50, global_step)\n",
    "        writer.add_scalar('val/ade_90', ade_90, global_step)\n",
    "        writer.add_scalar('val/fde_50', fde_50, global_step)\n",
    "        writer.add_scalar('val/fde_90', fde_90, global_step)\n",
    "        writer.add_scalar('val/eval_time', eval_time, global_step)\n",
    "        \n",
    "        # Add histograms of ADE and FDE\n",
    "        writer.add_histogram('val/ade_dist', np.array(all_ades), global_step)\n",
    "        writer.add_histogram('val/fde_dist', np.array(all_fdes), global_step)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'ade': avg_ade,\n",
    "        'fde': avg_fde,\n",
    "        'ade_50': ade_50,\n",
    "        'ade_90': ade_90,\n",
    "        'fde_50': fde_50,\n",
    "        'fde_90': fde_90,\n",
    "        'eval_time': eval_time,\n",
    "        'detailed_errors': all_errors\n",
    "    }\n",
    "\n",
    "# Prediction function with optional tensorboard visualizations\n",
    "def predict(model, test_loader, device, writer=None, visualize_samples=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    inference_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            past, mask = [x.to(device) for x in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(past, mask)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "            # Visualize sample predictions (only if requested and writer is provided)\n",
    "            if writer is not None and visualize_samples and batch_idx < 10:\n",
    "                # In a real implementation, you would generate and save figures\n",
    "                # writer.add_figure(f'test/trajectory_{batch_idx}', fig, 0)\n",
    "                pass\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    \n",
    "    # Log inference statistics if writer is provided\n",
    "    if writer is not None:\n",
    "        inference_time = time.time() - inference_start_time\n",
    "        avg_time_per_sample = inference_time / len(predictions)\n",
    "        writer.add_text('inference_stats', \n",
    "                      f\"Total inference time: {inference_time:.2f}s, \"\n",
    "                      f\"Samples: {len(predictions)}, \"\n",
    "                      f\"Avg time per sample: {avg_time_per_sample*1000:.2f}ms\")\n",
    "        \n",
    "        # Add histogram of prediction coordinates\n",
    "        writer.add_histogram('test/pred_x', predictions[:, :, 0].flatten(), 0)\n",
    "        writer.add_histogram('test/pred_y', predictions[:, :, 1].flatten(), 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e14e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = 'data/train.npz'\n",
    "test_input = 'data/test_input.npz'\n",
    "output_csv = 'predictions.csv'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir = f\"runs/simpl_{timestamp}\"\n",
    "ckpt_dir = os.path.join(\"checkpoints\", f\"simpl_{timestamp}\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "best_ckpt_path = os.path.join(ckpt_dir, \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe667e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "base_lr = 3e-5  # Lower learning rate for stability\n",
    "epochs = 200\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca8ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Log hyperparameters\n",
    "hparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'base_learning_rate': base_lr,\n",
    "    'epochs': epochs,\n",
    "    'model_type': 'SIMPL',\n",
    "    'd_model': 128,\n",
    "    'nhead': 8,\n",
    "    'num_layers_temporal': 3, \n",
    "    'num_layers_social': 2,\n",
    "    'polynomial_degree': 5,  \n",
    "    'dropout': 0.2,\n",
    "    'weight_decay': 1e-5,\n",
    "}\n",
    "writer.add_text('hyperparameters', str(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1c4275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Full data shape: (10000, 50, 110, 6)\n",
      "Position mean: [2830.48216443 1083.72035943], std: [3191.10358747 1637.55340973]\n",
      "Velocity mean: [-0.03593408 -0.02212871], std: [3.89127287 3.42483996]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "full_data = np.load(train_input)['data']\n",
    "print(f\"Full data shape: {full_data.shape}\")\n",
    "# Split into train and eval (7:3)\n",
    "num_samples = len(full_data)\n",
    "num_train = int(0.7 * num_samples)\n",
    "perm = np.random.permutation(num_samples)\n",
    "train_idx = perm[:num_train]\n",
    "eval_idx = perm[num_train:]\n",
    "\n",
    "train_data = full_data[train_idx]\n",
    "eval_data = full_data[eval_idx]\n",
    "\n",
    "# Calculate normalization statistics\n",
    "# Only consider non-zero values for better statistics\n",
    "mask = (full_data[..., :2] != 0).any(axis=-1)\n",
    "pos_all = full_data[..., :2][mask]\n",
    "vel_all = full_data[..., 2:4][mask]\n",
    "\n",
    "pos_mean = pos_all.mean(axis=0)\n",
    "pos_std = pos_all.std(axis=0) + 1e-6  # Add epsilon to avoid division by zero\n",
    "vel_mean = vel_all.mean(axis=0)\n",
    "vel_std = vel_all.std(axis=0) + 1e-6\n",
    "\n",
    "print(f\"Position mean: {pos_mean}, std: {pos_std}\")\n",
    "print(f\"Velocity mean: {vel_mean}, std: {vel_std}\")\n",
    "\n",
    "train_ds = TrajectoryDataset(\n",
    "    data=train_data,\n",
    "    pos_mean=pos_mean,\n",
    "    pos_std=pos_std,\n",
    "    vel_mean=vel_mean,\n",
    "    vel_std=vel_std)\n",
    "\n",
    "eval_ds = TrajectoryDataset(\n",
    "    data=eval_data,\n",
    "    pos_mean=pos_mean,\n",
    "    pos_std=pos_std,\n",
    "    vel_mean=vel_mean,\n",
    "    vel_std=vel_std\n",
    ")\n",
    "test_ds = TrajectoryDataset(\n",
    "    input_path=test_input,\n",
    "    is_test=True,\n",
    "    pos_mean=pos_mean,\n",
    "    pos_std=pos_std,\n",
    "    vel_mean=vel_mean,\n",
    "    vel_std=vel_std\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(eval_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Log dataset information\n",
    "writer.add_text('dataset_info', f\"Train samples: {len(train_ds)}, Eval samples: {len(eval_ds)}, Test samples: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d926097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model on device: cuda\n",
      "Bernstein basis shape: torch.Size([6, 60]), Pinv shape: torch.Size([6, 60])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating model on device: {device}\")\n",
    "\n",
    "# Important: use polynomial_degree=5 (not 6) to ensure compatibility\n",
    "polynomial_degree = 5\n",
    "\n",
    "model = SIMPLModel(\n",
    "    feature_dim=6,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers_temporal=3,  # Reduced from 4\n",
    "    num_layers_social=2,\n",
    "    dim_feedforward=256,\n",
    "    T_past=50,\n",
    "    T_future=60,\n",
    "    polynomial_degree=polynomial_degree,  # Match BernsteinLayer\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Log model architecture and parameters\n",
    "writer.add_text('model_architecture', str(model))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "writer.add_text('model_params', f\"Total trainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db2b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = configure_optimizer(model, base_lr=base_lr)\n",
    "\n",
    "# Create learning rate scheduler with warmup\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# One-cycle learning rate scheduler\n",
    "lr_scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=[base_lr*3, base_lr*4.5, base_lr*3.6, base_lr*1.5],  # Scale for each param group\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs,\n",
    "    pct_start=0.1,  # Spend 10% of time warming up\n",
    "    div_factor=25,  # Initial LR is max_lr/25\n",
    "    final_div_factor=1000,  # Final LR is max_lr/1000\n",
    "    anneal_strategy='cos'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3b6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Resuming from epoch 63 (val_loss=0.870934)\n"
     ]
    }
   ],
   "source": [
    "start_epoch   = 1\n",
    "best_val_loss = float('inf')\n",
    "global_step   = 0\n",
    "\n",
    "if os.path.exists(best_ckpt_path):\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    start_epoch   = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt.get('val_loss', best_val_loss)\n",
    "    global_step   = ckpt.get('global_step', 0)\n",
    "    hparams = ckpt.get('hparams', hparams)\n",
    "    print(f\"▶ Resuming from epoch {ckpt['epoch']} (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Log the training loop start\n",
    "writer.add_text('training_info', f\"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b94c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, epoch, model, optimizer, val_loss, val_metrics, global_step, hparams):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_metrics': val_metrics,\n",
    "        'global_step': global_step,\n",
    "        'hparams': hparams\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ee5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200 | Train Loss: 0.436582 | Val Loss: 0.867255 | Val ADE: 1.0623 | Val FDE: 1.0625 | Time: 22.57s\n",
      "✅ Best model saved at epoch 64 (val loss: 0.867255)\n",
      "Epoch 65/200 | Train Loss: 0.435433 | Val Loss: 0.866996 | Val ADE: 1.0638 | Val FDE: 1.0640 | Time: 20.79s\n",
      "✅ Best model saved at epoch 65 (val loss: 0.866996)\n",
      "Epoch 66/200 | Train Loss: 0.433848 | Val Loss: 0.866936 | Val ADE: 1.0632 | Val FDE: 1.0635 | Time: 20.87s\n",
      "✅ Best model saved at epoch 66 (val loss: 0.866936)\n",
      "Epoch 67/200 | Train Loss: 0.433048 | Val Loss: 0.866704 | Val ADE: 1.0638 | Val FDE: 1.0643 | Time: 20.90s\n",
      "✅ Best model saved at epoch 67 (val loss: 0.866704)\n",
      "Epoch 68/200 | Train Loss: 0.434678 | Val Loss: 0.865675 | Val ADE: 1.0625 | Val FDE: 1.0629 | Time: 20.87s\n",
      "✅ Best model saved at epoch 68 (val loss: 0.865675)\n",
      "Epoch 69/200 | Train Loss: 0.433444 | Val Loss: 0.867023 | Val ADE: 1.0641 | Val FDE: 1.0645 | Time: 21.19s\n",
      "No improvement for 1 epochs. Best val_loss: 0.865675\n",
      "Epoch 70/200 | Train Loss: 0.436056 | Val Loss: 0.869860 | Val ADE: 1.0696 | Val FDE: 1.0699 | Time: 21.11s\n",
      "No improvement for 2 epochs. Best val_loss: 0.865675\n",
      "Epoch 71/200 | Train Loss: 0.434161 | Val Loss: 0.866791 | Val ADE: 1.0623 | Val FDE: 1.0627 | Time: 21.20s\n",
      "No improvement for 3 epochs. Best val_loss: 0.865675\n",
      "Epoch 72/200 | Train Loss: 0.434290 | Val Loss: 0.868745 | Val ADE: 1.0672 | Val FDE: 1.0677 | Time: 20.97s\n",
      "No improvement for 4 epochs. Best val_loss: 0.865675\n",
      "Epoch 73/200 | Train Loss: 0.434330 | Val Loss: 0.867088 | Val ADE: 1.0620 | Val FDE: 1.0624 | Time: 20.96s\n",
      "No improvement for 5 epochs. Best val_loss: 0.865675\n",
      "Epoch 74/200 | Train Loss: 0.433970 | Val Loss: 0.869010 | Val ADE: 1.0619 | Val FDE: 1.0620 | Time: 20.94s\n",
      "No improvement for 6 epochs. Best val_loss: 0.865675\n",
      "Epoch 75/200 | Train Loss: 0.433965 | Val Loss: 0.864666 | Val ADE: 1.0610 | Val FDE: 1.0618 | Time: 20.97s\n",
      "✅ Best model saved at epoch 75 (val loss: 0.864666)\n",
      "Epoch 76/200 | Train Loss: 0.433414 | Val Loss: 0.867213 | Val ADE: 1.0613 | Val FDE: 1.0621 | Time: 20.95s\n",
      "No improvement for 1 epochs. Best val_loss: 0.864666\n",
      "Epoch 77/200 | Train Loss: 0.434262 | Val Loss: 0.874791 | Val ADE: 1.0753 | Val FDE: 1.0761 | Time: 20.97s\n",
      "No improvement for 2 epochs. Best val_loss: 0.864666\n",
      "Epoch 78/200 | Train Loss: 0.432654 | Val Loss: 0.872635 | Val ADE: 1.0651 | Val FDE: 1.0656 | Time: 21.00s\n",
      "No improvement for 3 epochs. Best val_loss: 0.864666\n",
      "Epoch 79/200 | Train Loss: 0.433801 | Val Loss: 0.871916 | Val ADE: 1.0643 | Val FDE: 1.0641 | Time: 20.96s\n",
      "No improvement for 4 epochs. Best val_loss: 0.864666\n",
      "Epoch 80/200 | Train Loss: 0.431402 | Val Loss: 0.871002 | Val ADE: 1.0589 | Val FDE: 1.0591 | Time: 20.96s\n",
      "No improvement for 5 epochs. Best val_loss: 0.864666\n",
      "🧪 Checkpoint saved at checkpoints/simpl_20250504_171027/epoch_80.pt\n",
      "Epoch 81/200 | Train Loss: 0.432736 | Val Loss: 0.872293 | Val ADE: 1.0652 | Val FDE: 1.0655 | Time: 20.96s\n",
      "No improvement for 6 epochs. Best val_loss: 0.864666\n",
      "Epoch 82/200 | Train Loss: 0.431909 | Val Loss: 0.870898 | Val ADE: 1.0679 | Val FDE: 1.0678 | Time: 20.96s\n",
      "No improvement for 7 epochs. Best val_loss: 0.864666\n",
      "Epoch 83/200 | Train Loss: 0.432045 | Val Loss: 0.874295 | Val ADE: 1.0746 | Val FDE: 1.0736 | Time: 20.93s\n",
      "No improvement for 8 epochs. Best val_loss: 0.864666\n",
      "Epoch 84/200 | Train Loss: 0.431221 | Val Loss: 0.878449 | Val ADE: 1.0647 | Val FDE: 1.0652 | Time: 21.01s\n",
      "No improvement for 9 epochs. Best val_loss: 0.864666\n",
      "Epoch 85/200 | Train Loss: 0.431533 | Val Loss: 0.872207 | Val ADE: 1.0609 | Val FDE: 1.0610 | Time: 20.97s\n",
      "No improvement for 10 epochs. Best val_loss: 0.864666\n",
      "Epoch 86/200 | Train Loss: 0.427753 | Val Loss: 0.871504 | Val ADE: 1.0643 | Val FDE: 1.0645 | Time: 21.00s\n",
      "No improvement for 11 epochs. Best val_loss: 0.864666\n",
      "Epoch 87/200 | Train Loss: 0.429218 | Val Loss: 0.876552 | Val ADE: 1.0637 | Val FDE: 1.0649 | Time: 20.91s\n",
      "No improvement for 12 epochs. Best val_loss: 0.864666\n",
      "Epoch 88/200 | Train Loss: 0.428909 | Val Loss: 0.867549 | Val ADE: 1.0647 | Val FDE: 1.0646 | Time: 21.01s\n",
      "No improvement for 13 epochs. Best val_loss: 0.864666\n",
      "Epoch 89/200 | Train Loss: 0.429065 | Val Loss: 0.869065 | Val ADE: 1.0590 | Val FDE: 1.0589 | Time: 20.97s\n",
      "No improvement for 14 epochs. Best val_loss: 0.864666\n",
      "Epoch 90/200 | Train Loss: 0.428361 | Val Loss: 0.872999 | Val ADE: 1.0590 | Val FDE: 1.0583 | Time: 21.00s\n",
      "No improvement for 15 epochs. Best val_loss: 0.864666\n",
      "Epoch 91/200 | Train Loss: 0.425558 | Val Loss: 0.883224 | Val ADE: 1.0581 | Val FDE: 1.0577 | Time: 20.99s\n",
      "No improvement for 16 epochs. Best val_loss: 0.864666\n",
      "Epoch 92/200 | Train Loss: 0.426654 | Val Loss: 0.869713 | Val ADE: 1.0570 | Val FDE: 1.0563 | Time: 20.96s\n",
      "No improvement for 17 epochs. Best val_loss: 0.864666\n",
      "Epoch 93/200 | Train Loss: 0.424364 | Val Loss: 0.877192 | Val ADE: 1.0604 | Val FDE: 1.0602 | Time: 21.08s\n",
      "No improvement for 18 epochs. Best val_loss: 0.864666\n",
      "Epoch 94/200 | Train Loss: 0.425837 | Val Loss: 0.881539 | Val ADE: 1.0572 | Val FDE: 1.0573 | Time: 21.05s\n",
      "No improvement for 19 epochs. Best val_loss: 0.864666\n",
      "Epoch 95/200 | Train Loss: 0.426373 | Val Loss: 0.877650 | Val ADE: 1.0658 | Val FDE: 1.0649 | Time: 21.06s\n",
      "No improvement for 20 epochs. Best val_loss: 0.864666\n",
      "Epoch 96/200 | Train Loss: 0.427228 | Val Loss: 0.867798 | Val ADE: 1.0618 | Val FDE: 1.0617 | Time: 21.01s\n",
      "No improvement for 21 epochs. Best val_loss: 0.864666\n",
      "Epoch 97/200 | Train Loss: 0.424716 | Val Loss: 0.870052 | Val ADE: 1.0573 | Val FDE: 1.0565 | Time: 21.12s\n",
      "No improvement for 22 epochs. Best val_loss: 0.864666\n",
      "Epoch 98/200 | Train Loss: 0.424710 | Val Loss: 0.874410 | Val ADE: 1.0661 | Val FDE: 1.0650 | Time: 21.04s\n",
      "No improvement for 23 epochs. Best val_loss: 0.864666\n",
      "Epoch 99/200 | Train Loss: 0.420210 | Val Loss: 0.869896 | Val ADE: 1.0662 | Val FDE: 1.0656 | Time: 21.08s\n",
      "No improvement for 24 epochs. Best val_loss: 0.864666\n",
      "Epoch 100/200 | Train Loss: 0.422849 | Val Loss: 0.864929 | Val ADE: 1.0569 | Val FDE: 1.0561 | Time: 21.05s\n",
      "No improvement for 25 epochs. Best val_loss: 0.864666\n",
      "🧪 Checkpoint saved at checkpoints/simpl_20250504_171027/epoch_100.pt\n",
      "Epoch 101/200 | Train Loss: 0.423021 | Val Loss: 0.868372 | Val ADE: 1.0597 | Val FDE: 1.0587 | Time: 21.06s\n",
      "No improvement for 26 epochs. Best val_loss: 0.864666\n",
      "Epoch 102/200 | Train Loss: 0.421695 | Val Loss: 0.869470 | Val ADE: 1.0674 | Val FDE: 1.0658 | Time: 20.92s\n",
      "No improvement for 27 epochs. Best val_loss: 0.864666\n",
      "Epoch 103/200 | Train Loss: 0.421029 | Val Loss: 0.869896 | Val ADE: 1.0638 | Val FDE: 1.0624 | Time: 20.93s\n",
      "No improvement for 28 epochs. Best val_loss: 0.864666\n",
      "Epoch 104/200 | Train Loss: 0.420617 | Val Loss: 0.871735 | Val ADE: 1.0569 | Val FDE: 1.0563 | Time: 21.01s\n",
      "No improvement for 29 epochs. Best val_loss: 0.864666\n",
      "Epoch 105/200 | Train Loss: 0.419581 | Val Loss: 0.875261 | Val ADE: 1.0561 | Val FDE: 1.0556 | Time: 20.95s\n",
      "No improvement for 30 epochs. Best val_loss: 0.864666\n",
      "Epoch 106/200 | Train Loss: 0.420679 | Val Loss: 0.874808 | Val ADE: 1.0604 | Val FDE: 1.0601 | Time: 20.97s\n",
      "No improvement for 31 epochs. Best val_loss: 0.864666\n",
      "Epoch 107/200 | Train Loss: 0.420587 | Val Loss: 0.863190 | Val ADE: 1.0639 | Val FDE: 1.0638 | Time: 21.02s\n",
      "✅ Best model saved at epoch 107 (val loss: 0.863190)\n",
      "Epoch 108/200 | Train Loss: 0.418411 | Val Loss: 0.873309 | Val ADE: 1.0541 | Val FDE: 1.0540 | Time: 21.01s\n",
      "No improvement for 1 epochs. Best val_loss: 0.863190\n",
      "Epoch 109/200 | Train Loss: 0.417075 | Val Loss: 0.872169 | Val ADE: 1.0568 | Val FDE: 1.0571 | Time: 21.03s\n",
      "No improvement for 2 epochs. Best val_loss: 0.863190\n",
      "Epoch 110/200 | Train Loss: 0.415595 | Val Loss: 0.875015 | Val ADE: 1.0516 | Val FDE: 1.0520 | Time: 21.04s\n",
      "No improvement for 3 epochs. Best val_loss: 0.863190\n",
      "Epoch 111/200 | Train Loss: 0.421420 | Val Loss: 0.873843 | Val ADE: 1.0539 | Val FDE: 1.0535 | Time: 21.09s\n",
      "No improvement for 4 epochs. Best val_loss: 0.863190\n",
      "Epoch 112/200 | Train Loss: 0.416253 | Val Loss: 0.871461 | Val ADE: 1.0618 | Val FDE: 1.0614 | Time: 21.03s\n",
      "No improvement for 5 epochs. Best val_loss: 0.863190\n",
      "Epoch 113/200 | Train Loss: 0.416247 | Val Loss: 0.868016 | Val ADE: 1.0562 | Val FDE: 1.0567 | Time: 21.13s\n",
      "No improvement for 6 epochs. Best val_loss: 0.863190\n",
      "Epoch 114/200 | Train Loss: 0.417971 | Val Loss: 0.865770 | Val ADE: 1.0574 | Val FDE: 1.0582 | Time: 20.96s\n",
      "No improvement for 7 epochs. Best val_loss: 0.863190\n",
      "Epoch 115/200 | Train Loss: 0.415163 | Val Loss: 0.874931 | Val ADE: 1.0560 | Val FDE: 1.0557 | Time: 21.14s\n",
      "No improvement for 8 epochs. Best val_loss: 0.863190\n",
      "Epoch 116/200 | Train Loss: 0.416234 | Val Loss: 0.867224 | Val ADE: 1.0599 | Val FDE: 1.0600 | Time: 20.62s\n",
      "No improvement for 9 epochs. Best val_loss: 0.863190\n",
      "Epoch 117/200 | Train Loss: 0.412237 | Val Loss: 0.874616 | Val ADE: 1.0540 | Val FDE: 1.0536 | Time: 20.55s\n",
      "No improvement for 10 epochs. Best val_loss: 0.863190\n",
      "Epoch 118/200 | Train Loss: 0.414499 | Val Loss: 0.861630 | Val ADE: 1.0566 | Val FDE: 1.0572 | Time: 20.56s\n",
      "✅ Best model saved at epoch 118 (val loss: 0.861630)\n",
      "Epoch 119/200 | Train Loss: 0.411141 | Val Loss: 0.870728 | Val ADE: 1.0522 | Val FDE: 1.0517 | Time: 20.62s\n",
      "No improvement for 1 epochs. Best val_loss: 0.861630\n",
      "Epoch 120/200 | Train Loss: 0.413916 | Val Loss: 0.874812 | Val ADE: 1.0523 | Val FDE: 1.0525 | Time: 20.68s\n",
      "No improvement for 2 epochs. Best val_loss: 0.861630\n",
      "🧪 Checkpoint saved at checkpoints/simpl_20250504_171027/epoch_120.pt\n",
      "Epoch 121/200 | Train Loss: 0.409723 | Val Loss: 0.871009 | Val ADE: 1.0663 | Val FDE: 1.0664 | Time: 20.65s\n",
      "No improvement for 3 epochs. Best val_loss: 0.861630\n",
      "Epoch 122/200 | Train Loss: 0.412886 | Val Loss: 0.870317 | Val ADE: 1.0538 | Val FDE: 1.0545 | Time: 20.70s\n",
      "No improvement for 4 epochs. Best val_loss: 0.861630\n",
      "Epoch 123/200 | Train Loss: 0.410084 | Val Loss: 0.871282 | Val ADE: 1.0513 | Val FDE: 1.0514 | Time: 20.69s\n",
      "No improvement for 5 epochs. Best val_loss: 0.861630\n",
      "Epoch 124/200 | Train Loss: 0.409082 | Val Loss: 0.877922 | Val ADE: 1.0633 | Val FDE: 1.0630 | Time: 20.71s\n",
      "No improvement for 6 epochs. Best val_loss: 0.861630\n",
      "Epoch 125/200 | Train Loss: 0.410914 | Val Loss: 0.873777 | Val ADE: 1.0560 | Val FDE: 1.0557 | Time: 20.69s\n",
      "No improvement for 7 epochs. Best val_loss: 0.861630\n",
      "Epoch 126/200 | Train Loss: 0.408157 | Val Loss: 0.867991 | Val ADE: 1.0528 | Val FDE: 1.0524 | Time: 20.71s\n",
      "No improvement for 8 epochs. Best val_loss: 0.861630\n",
      "Epoch 127/200 | Train Loss: 0.409282 | Val Loss: 0.868691 | Val ADE: 1.0514 | Val FDE: 1.0510 | Time: 20.68s\n",
      "No improvement for 9 epochs. Best val_loss: 0.861630\n",
      "Epoch 128/200 | Train Loss: 0.407758 | Val Loss: 0.873126 | Val ADE: 1.0537 | Val FDE: 1.0532 | Time: 20.69s\n",
      "No improvement for 10 epochs. Best val_loss: 0.861630\n",
      "Epoch 129/200 | Train Loss: 0.408521 | Val Loss: 0.877674 | Val ADE: 1.0551 | Val FDE: 1.0551 | Time: 20.66s\n",
      "No improvement for 11 epochs. Best val_loss: 0.861630\n",
      "Epoch 130/200 | Train Loss: 0.405385 | Val Loss: 0.874598 | Val ADE: 1.0594 | Val FDE: 1.0591 | Time: 20.73s\n",
      "No improvement for 12 epochs. Best val_loss: 0.861630\n",
      "Epoch 131/200 | Train Loss: 0.406371 | Val Loss: 0.886444 | Val ADE: 1.0545 | Val FDE: 1.0545 | Time: 20.63s\n",
      "No improvement for 13 epochs. Best val_loss: 0.861630\n",
      "Epoch 132/200 | Train Loss: 0.406687 | Val Loss: 0.875582 | Val ADE: 1.0531 | Val FDE: 1.0529 | Time: 20.69s\n",
      "No improvement for 14 epochs. Best val_loss: 0.861630\n",
      "Epoch 133/200 | Train Loss: 0.406653 | Val Loss: 0.875401 | Val ADE: 1.0633 | Val FDE: 1.0631 | Time: 20.69s\n",
      "No improvement for 15 epochs. Best val_loss: 0.861630\n",
      "Epoch 134/200 | Train Loss: 0.404485 | Val Loss: 0.883394 | Val ADE: 1.0624 | Val FDE: 1.0624 | Time: 20.66s\n",
      "No improvement for 16 epochs. Best val_loss: 0.861630\n",
      "Epoch 135/200 | Train Loss: 0.405811 | Val Loss: 0.878992 | Val ADE: 1.0532 | Val FDE: 1.0527 | Time: 20.71s\n",
      "No improvement for 17 epochs. Best val_loss: 0.861630\n",
      "Epoch 136/200 | Train Loss: 0.406763 | Val Loss: 0.878352 | Val ADE: 1.0559 | Val FDE: 1.0555 | Time: 20.66s\n",
      "No improvement for 18 epochs. Best val_loss: 0.861630\n",
      "Epoch 137/200 | Train Loss: 0.404157 | Val Loss: 0.869987 | Val ADE: 1.0550 | Val FDE: 1.0548 | Time: 20.66s\n",
      "No improvement for 19 epochs. Best val_loss: 0.861630\n",
      "Epoch 138/200 | Train Loss: 0.404950 | Val Loss: 0.870354 | Val ADE: 1.0544 | Val FDE: 1.0534 | Time: 20.68s\n",
      "No improvement for 20 epochs. Best val_loss: 0.861630\n",
      "Epoch 139/200 | Train Loss: 0.400761 | Val Loss: 0.892747 | Val ADE: 1.0548 | Val FDE: 1.0541 | Time: 20.71s\n",
      "No improvement for 21 epochs. Best val_loss: 0.861630\n",
      "Epoch 140/200 | Train Loss: 0.402010 | Val Loss: 0.873042 | Val ADE: 1.0539 | Val FDE: 1.0535 | Time: 20.68s\n",
      "No improvement for 22 epochs. Best val_loss: 0.861630\n",
      "🧪 Checkpoint saved at checkpoints/simpl_20250504_171027/epoch_140.pt\n",
      "Epoch 141/200 | Train Loss: 0.403614 | Val Loss: 0.874542 | Val ADE: 1.0623 | Val FDE: 1.0625 | Time: 20.67s\n",
      "No improvement for 23 epochs. Best val_loss: 0.861630\n",
      "Epoch 142/200 | Train Loss: 0.402660 | Val Loss: 0.884986 | Val ADE: 1.0511 | Val FDE: 1.0506 | Time: 20.63s\n",
      "No improvement for 24 epochs. Best val_loss: 0.861630\n",
      "Epoch 143/200 | Train Loss: 0.401304 | Val Loss: 0.872273 | Val ADE: 1.0524 | Val FDE: 1.0520 | Time: 20.67s\n",
      "No improvement for 25 epochs. Best val_loss: 0.861630\n",
      "Epoch 144/200 | Train Loss: 0.399400 | Val Loss: 0.872196 | Val ADE: 1.0499 | Val FDE: 1.0500 | Time: 20.69s\n",
      "No improvement for 26 epochs. Best val_loss: 0.861630\n",
      "Epoch 145/200 | Train Loss: 0.402562 | Val Loss: 0.880413 | Val ADE: 1.0519 | Val FDE: 1.0522 | Time: 20.70s\n",
      "No improvement for 27 epochs. Best val_loss: 0.861630\n",
      "Epoch 146/200 | Train Loss: 0.401637 | Val Loss: 0.874293 | Val ADE: 1.0492 | Val FDE: 1.0493 | Time: 20.72s\n",
      "No improvement for 28 epochs. Best val_loss: 0.861630\n",
      "Epoch 147/200 | Train Loss: 0.401402 | Val Loss: 0.882582 | Val ADE: 1.0596 | Val FDE: 1.0593 | Time: 20.65s\n",
      "No improvement for 29 epochs. Best val_loss: 0.861630\n",
      "Epoch 148/200 | Train Loss: 0.397626 | Val Loss: 0.877852 | Val ADE: 1.0521 | Val FDE: 1.0520 | Time: 20.68s\n",
      "No improvement for 30 epochs. Best val_loss: 0.861630\n",
      "Epoch 149/200 | Train Loss: 0.396702 | Val Loss: 0.876281 | Val ADE: 1.0537 | Val FDE: 1.0536 | Time: 20.67s\n",
      "No improvement for 31 epochs. Best val_loss: 0.861630\n",
      "Epoch 150/200 | Train Loss: 0.397666 | Val Loss: 0.872541 | Val ADE: 1.0494 | Val FDE: 1.0497 | Time: 20.73s\n",
      "No improvement for 32 epochs. Best val_loss: 0.861630\n",
      "Epoch 151/200 | Train Loss: 0.395761 | Val Loss: 0.884958 | Val ADE: 1.0564 | Val FDE: 1.0563 | Time: 20.62s\n",
      "No improvement for 33 epochs. Best val_loss: 0.861630\n",
      "Epoch 152/200 | Train Loss: 0.398253 | Val Loss: 0.885932 | Val ADE: 1.0517 | Val FDE: 1.0515 | Time: 20.70s\n",
      "No improvement for 34 epochs. Best val_loss: 0.861630\n",
      "Epoch 153/200 | Train Loss: 0.394968 | Val Loss: 0.869855 | Val ADE: 1.0491 | Val FDE: 1.0490 | Time: 20.62s\n",
      "No improvement for 35 epochs. Best val_loss: 0.861630\n",
      "Epoch 154/200 | Train Loss: 0.398021 | Val Loss: 0.878375 | Val ADE: 1.0511 | Val FDE: 1.0508 | Time: 20.73s\n",
      "No improvement for 36 epochs. Best val_loss: 0.861630\n",
      "Training interrupted by user\n",
      "Total training time: 0h 35m 9s\n",
      "Final model saved at checkpoints/simpl_20250504_171027/final_model.pt\n"
     ]
    }
   ],
   "source": [
    "best_val_metrics = {}\n",
    "try:\n",
    "    # Initialize early stopping counter\n",
    "    patience = 50\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train for one epoch\n",
    "        model, global_step, train_loss = train(\n",
    "            model, train_loader, optimizer, device,\n",
    "            epoch=epoch,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            writer=writer,\n",
    "            global_step=global_step\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(\n",
    "            model, eval_loader, device, \n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "        val_loss = val_metrics['loss']\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.6f} | \"\n",
    "              f\"Val Loss: {val_loss:.6f} | \"\n",
    "              f\"Val ADE: {val_metrics['ade']:.4f} | \"\n",
    "              f\"Val FDE: {val_metrics['fde']:.4f} | \"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_metrics = val_metrics\n",
    "            save_checkpoint(\n",
    "                best_ckpt_path, epoch, model, optimizer,\n",
    "                val_loss, val_metrics, global_step, hparams\n",
    "            )\n",
    "            print(f\"✅ Best model saved at epoch {epoch} (val loss: {best_val_loss:.6f})\")\n",
    "            writer.add_text('checkpoints', f\"New best model at epoch {epoch} with val_loss: {best_val_loss:.6f}\")\n",
    "            # Reset early stopping counter\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            # Increment early stopping counter\n",
    "            early_stop_counter += 1\n",
    "            print(f\"No improvement for {early_stop_counter} epochs. Best val_loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            cur_ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch}.pt\")\n",
    "            save_checkpoint(\n",
    "                cur_ckpt_path, epoch, model, optimizer,\n",
    "                val_loss, val_metrics, global_step, hparams\n",
    "            )\n",
    "            print(f\"🧪 Checkpoint saved at {cur_ckpt_path}\")\n",
    "            writer.add_text('checkpoints', f\"Periodic checkpoint at epoch {epoch}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "            writer.add_text('training_info', f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")\n",
    "    writer.add_text('training_info', f\"Training interrupted at epoch {epoch}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    writer.add_text('training_info', f\"Training crashed with error: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Calculate total training time\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    hours, remainder = divmod(total_training_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    time_str = f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\"\n",
    "    \n",
    "    print(f\"Total training time: {time_str}\")\n",
    "    writer.add_text('training_info', f\"Training completed/interrupted after {time_str}\")\n",
    "    \n",
    "    # Save final model if different from best model\n",
    "    if not os.path.exists(os.path.join(ckpt_dir, \"final_model.pt\")):\n",
    "        final_path = os.path.join(ckpt_dir, \"final_model.pt\")\n",
    "        # use best_val_loss/metrics if val_loss isn’t in scope\n",
    "        vloss = locals().get('val_loss', best_val_loss)\n",
    "        vmetrics = locals().get('val_metrics', best_val_metrics)\n",
    "        try:\n",
    "            save_checkpoint(final_path, epoch, model, optimizer,\n",
    "                            vloss, vmetrics, global_step, hparams)\n",
    "            print(f\"Final model saved at {final_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save final model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for prediction...\n",
      "Loaded model from epoch 118 with val_loss=0.861630\n",
      "Generating predictions on test set...\n",
      "Denormalizing predictions...\n",
      "Creating submission file: predictions.csv\n",
      "Predictions saved to predictions.csv\n",
      "Training completed!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Loading best model for prediction...\")\n",
    "try:\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=device,weights_only=False)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {ckpt['epoch']} with val_loss={ckpt['val_loss']:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load best model: {e}\")\n",
    "    print(\"Using current model instead\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions on test set...\")\n",
    "preds = predict(model, test_loader, device, writer=writer)\n",
    "\n",
    "# Denormalize predictions if normalization was applied\n",
    "if hasattr(test_ds, 'pos_mean') and test_ds.pos_mean is not None:\n",
    "    print(\"Denormalizing predictions...\")\n",
    "    preds_denorm = preds * test_ds.pos_std + test_ds.pos_mean\n",
    "else:\n",
    "    preds_denorm = preds\n",
    "\n",
    "# Reshape to format expected by the competition\n",
    "preds_flat = preds_denorm.reshape(-1, 2)\n",
    "\n",
    "# Create submission file\n",
    "print(f\"Creating submission file: {output_csv}\")\n",
    "import pandas as pd\n",
    "df_preds = pd.DataFrame(preds_flat, columns=['x', 'y'])\n",
    "df_preds.index.name = 'ID'\n",
    "df_preds.to_csv(output_csv)\n",
    "print(f\"Predictions saved to {output_csv}\")\n",
    "\n",
    "# Close tensorboard writer\n",
    "writer.close()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5395b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
