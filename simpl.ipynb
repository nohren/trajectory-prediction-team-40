{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca95b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a1a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 3:  # Training data with future\n",
    "        pasts, masks, futures = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        future = torch.stack(futures)\n",
    "        return past, mask, future\n",
    "    else:  # Test data without future\n",
    "        pasts, masks = zip(*batch)\n",
    "        past = torch.stack(pasts)\n",
    "        mask = torch.stack(masks)\n",
    "        return past, mask\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, input_path=None, data=None, T_past=50, T_future=60, is_test=False):\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        else:\n",
    "            npz = np.load(input_path)\n",
    "            self.data = npz['data']\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]  # (num_agents, T, 6)\n",
    "        past = scene[:, :self.T_past, :]\n",
    "        mask = np.sum(np.abs(past[..., :2]), axis=(1, 2)) > 0\n",
    "        if not self.is_test and scene.shape[1] >= self.T_past + self.T_future:\n",
    "            future = scene[0, self.T_past:self.T_past + self.T_future, :2]\n",
    "            return (\n",
    "                torch.tensor(past, dtype=torch.float32),\n",
    "                torch.tensor(mask, dtype=torch.bool),\n",
    "                torch.tensor(future, dtype=torch.float32)\n",
    "            )\n",
    "        return (\n",
    "            torch.tensor(past, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.bool)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a741e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernsteinLayer(nn.Module):\n",
    "    def __init__(self, n_degree, T=60):\n",
    "        super().__init__()\n",
    "        self.n_degree = n_degree\n",
    "        self.T = T\n",
    "        self._precompute_bernstein_values()\n",
    "\n",
    "    def _precompute_bernstein_values(self):\n",
    "        time_points = torch.linspace(0, 1, self.T)\n",
    "        bern = torch.zeros(self.n_degree + 1, self.T)\n",
    "        for i in range(self.n_degree + 1):\n",
    "            binom = math.comb(self.n_degree, i)\n",
    "            bern[i] = binom * (time_points ** i) * ((1 - time_points) ** (self.n_degree - i))\n",
    "        # register as (n+1, T)\n",
    "        self.register_buffer('bernstein_values', bern)\n",
    "\n",
    "    def forward(self, control_points):\n",
    "        # control_points: (B, n+1, 2)\n",
    "        cp = control_points.transpose(1, 2)  # (B, 2, n+1)\n",
    "        # matmul broadcasts: (B,2,n+1) @ (n+1,T) -> (B,2,T)\n",
    "        traj = torch.matmul(cp, self.bernstein_values)  # (B,2,T)\n",
    "        trajectories = traj.transpose(1, 2)  # (B,T,2)\n",
    "        return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0075f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pos_enc = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        # shape (1, max_len, d_model)\n",
    "        pos_enc = pos_enc.unsqueeze(0)\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pos_enc[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8f59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rpe_processor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, source, target, rpe=None):\n",
    "        B, N_source, D = source.shape\n",
    "        _, N_target, _ = target.shape\n",
    "        query = self.query_proj(target)\n",
    "        key = self.key_proj(source)\n",
    "        value = self.value_proj(source)\n",
    "        if rpe is not None:\n",
    "            proc = self.rpe_processor(rpe)\n",
    "            enhanced = key.unsqueeze(1).repeat(1, N_target, 1, 1) + proc\n",
    "            outputs = []\n",
    "            for i in range(N_target):\n",
    "                q = query[:, i:i+1, :]\n",
    "                k = enhanced[:, i, :, :]\n",
    "                v = value\n",
    "                out, _ = self.multihead_attn(q, k, v)\n",
    "                outputs.append(out)\n",
    "            attn_output = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            attn_output, _ = self.multihead_attn(query, key, value)\n",
    "        target = self.norm1(target + self.dropout(attn_output))\n",
    "        ff_out = self.ff(target)\n",
    "        return self.norm2(target + self.dropout(ff_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0a6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIMPLModel(nn.Module):\n",
    "    def __init__(self, feature_dim=6, d_model=128, nhead=8,\n",
    "                 num_layers_temporal=2, num_layers_social=2,\n",
    "                 dim_feedforward=256, T_past=50, T_future=60,\n",
    "                 polynomial_degree=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.T_past = T_past\n",
    "        self.T_future = T_future\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.input_embed = nn.Linear(feature_dim, d_model)\n",
    "        self.time_pos_enc = PositionalEncoding(d_model, max_len=T_past)\n",
    "        self.temporal_encoders = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "            for _ in range(num_layers_temporal)\n",
    "        ])\n",
    "        self.rpe_generator = nn.Sequential(\n",
    "            nn.Linear(feature_dim, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // 2, d_model)\n",
    "        )\n",
    "        self.social_encoders = nn.ModuleList([\n",
    "            SymmetricAttention(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers_social)\n",
    "        ])\n",
    "        self.control_point_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, dim_feedforward // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward // 2, 2 * (polynomial_degree + 1))\n",
    "        )\n",
    "        self.bernstein_layer = BernsteinLayer(polynomial_degree, T_future)\n",
    "\n",
    "    def compute_relative_position_embedding(self, past, mask):\n",
    "        B, N, T, F = past.shape\n",
    "        last = past[:, :, -1, :]\n",
    "        ego = last[:, 0:1, :].expand(-1, N, -1)\n",
    "        rpe_feats = ego - last\n",
    "        rpe_feats = rpe_feats * mask.unsqueeze(-1).float()\n",
    "        return self.rpe_generator(rpe_feats)\n",
    "\n",
    "    def forward(self, past, agent_mask):\n",
    "        B, N, T, F = past.shape\n",
    "        # always include ego agent\n",
    "        agent_mask = agent_mask.clone()\n",
    "        agent_mask[:, 0] = True\n",
    "        # temporal embedding\n",
    "        x = past.view(B * N, T, F)\n",
    "        x = self.input_embed(x)\n",
    "        x = x / (x.norm(dim=-1, keepdim=True) + 1e-6) * math.sqrt(self.d_model)\n",
    "        x = self.time_pos_enc(x)\n",
    "        for layer in self.temporal_encoders:\n",
    "            x = layer(x)\n",
    "        # pool final state\n",
    "        x = x[:, -1, :]\n",
    "        agent_feats = x.view(B, N, self.d_model)\n",
    "        # social interaction\n",
    "        rpe = self.compute_relative_position_embedding(past, agent_mask)\n",
    "        ego_feats = agent_feats[:, 0:1, :]\n",
    "        others = agent_feats\n",
    "        rpe_mat = rpe.unsqueeze(1)\n",
    "        for layer in self.social_encoders:\n",
    "            ego_feats = layer(others, ego_feats, rpe_mat)\n",
    "        ego_embed = ego_feats.squeeze(1)\n",
    "        cps_flat = self.control_point_predictor(ego_embed)\n",
    "        cps = cps_flat.view(B, self.polynomial_degree + 1, 2)\n",
    "        return self.bernstein_layer(cps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a2d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device, num_epochs=10, lr_scheduler=None, writer=None, global_step=0):\n",
    "    model.train()\n",
    "    position_criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        past, mask, future = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Check for NaNs or Infs in inputs\n",
    "        if torch.isnan(past).any() or torch.isinf(past).any():\n",
    "            print(\"Warning: NaN or Inf detected in past input. Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        if torch.isnan(future).any() or torch.isinf(future).any():\n",
    "            print(\"Warning: NaN or Inf detected in future target. Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(past, mask)\n",
    "        \n",
    "        # Check for NaNs or Infs in predictions\n",
    "        if torch.isnan(pred).any() or torch.isinf(pred).any():\n",
    "            print(\"Warning: NaN or Inf detected in predictions. Skipping batch.\")\n",
    "            continue\n",
    "        \n",
    "        # Position loss (smooth L1)\n",
    "        loss = position_criterion(pred, future)\n",
    "        \n",
    "        # Calculate ADE for monitoring\n",
    "        with torch.no_grad():\n",
    "            mse = torch.pow(pred - future, 2).sum(dim=2)  # (B, T)\n",
    "            ade = torch.sqrt(mse).mean(dim=1).mean()  # scalar\n",
    "            fde = torch.sqrt(mse[:, -1]).mean()  # scalar\n",
    "        \n",
    "        # Backward pass with gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss * past.size(0)\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Log metrics to tensorboard every 20 batches\n",
    "        if writer is not None and batch_idx % 20 == 0:\n",
    "            writer.add_scalar('train/batch_loss', batch_loss, global_step)\n",
    "            writer.add_scalar('train/batch_ade', ade.item(), global_step)\n",
    "            writer.add_scalar('train/batch_fde', fde.item(), global_step)\n",
    "            writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "            \n",
    "            \n",
    "            # Log histograms of model weights and gradients\n",
    "            if batch_idx % 200 == 0:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        writer.add_histogram(f'weights/{name}', param.data, global_step)\n",
    "                        if param.grad is not None:\n",
    "                            writer.add_histogram(f'gradients/{name}', param.grad, global_step)\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    # Step learning rate scheduler if provided\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader.dataset)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('train/epoch_loss', epoch_loss, global_step)\n",
    "        writer.add_scalar('train/epoch_time', epoch_time, global_step)\n",
    "    \n",
    "    print(f\"Training - Loss: {epoch_loss:.6f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    return model, global_step\n",
    "\n",
    "# Evaluation function with tensorboard logging\n",
    "def evaluate(model, val_loader, device, writer=None, global_step=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    mse_criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    all_errors = []\n",
    "    all_ades = []\n",
    "    all_fdes = []\n",
    "    \n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            past, mask, future = [x.to(device) for x in batch]\n",
    "            pred = model(past, mask)\n",
    "            \n",
    "            # Calculate MSE loss per time step and sample\n",
    "            mse = mse_criterion(pred, future)  # (B, T, 2)\n",
    "            \n",
    "            # Calculate ADE (Average Displacement Error)\n",
    "            ade = torch.sqrt(mse.sum(dim=2)).mean(dim=1)  # (B,)\n",
    "            all_ades.extend(ade.cpu().numpy())\n",
    "            \n",
    "            # Calculate FDE (Final Displacement Error)\n",
    "            fde = torch.sqrt(mse[:, -1].sum(dim=1))  # (B,)\n",
    "            all_fdes.extend(fde.cpu().numpy())\n",
    "            \n",
    "            # Store errors for reporting\n",
    "            for i in range(len(ade)):\n",
    "                all_errors.append({\n",
    "                    'ade': ade[i].item(),\n",
    "                    'fde': fde[i].item()\n",
    "                })\n",
    "            \n",
    "            # Accumulate MSE loss\n",
    "            batch_loss = mse.mean()\n",
    "            total_loss += batch_loss.item() * past.size(0)\n",
    "            \n",
    "            # Log sample trajectories periodically\n",
    "            if writer is not None and batch_idx % 50 == 0 and batch_idx < 150:\n",
    "                # Plot sample trajectories for validation\n",
    "                for i in range(min(2, past.size(0))):\n",
    "                    fig_path = f'val_sample_{i}_step_{global_step}.png'\n",
    "                    # In a real implementation, you'd visualize trajectories here\n",
    "                    # writer.add_figure(f'val/trajectory_{i}', fig, global_step)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    avg_ade = np.mean(all_ades)\n",
    "    avg_fde = np.mean(all_fdes)\n",
    "    \n",
    "    # Calculate additional metrics - percentiles\n",
    "    ade_50 = np.percentile(all_ades, 50)  # median\n",
    "    ade_90 = np.percentile(all_ades, 90)  # 90th percentile\n",
    "    fde_50 = np.percentile(all_fdes, 50)  # median\n",
    "    fde_90 = np.percentile(all_fdes, 90)  # 90th percentile\n",
    "    \n",
    "    eval_time = time.time() - eval_start_time\n",
    "    \n",
    "    # Log metrics to tensorboard\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar('val/loss', avg_loss, global_step)\n",
    "        writer.add_scalar('val/ade_mean', avg_ade, global_step)\n",
    "        writer.add_scalar('val/fde_mean', avg_fde, global_step)\n",
    "        writer.add_scalar('val/ade_50', ade_50, global_step)\n",
    "        writer.add_scalar('val/ade_90', ade_90, global_step)\n",
    "        writer.add_scalar('val/fde_50', fde_50, global_step)\n",
    "        writer.add_scalar('val/fde_90', fde_90, global_step)\n",
    "        writer.add_scalar('val/eval_time', eval_time, global_step)\n",
    "        \n",
    "        # Add histograms of ADE and FDE\n",
    "        writer.add_histogram('val/ade_dist', np.array(all_ades), global_step)\n",
    "        writer.add_histogram('val/fde_dist', np.array(all_fdes), global_step)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'ade': avg_ade,\n",
    "        'fde': avg_fde,\n",
    "        'ade_50': ade_50,\n",
    "        'ade_90': ade_90,\n",
    "        'fde_50': fde_50,\n",
    "        'fde_90': fde_90,\n",
    "        'eval_time': eval_time,\n",
    "        'detailed_errors': all_errors\n",
    "    }\n",
    "\n",
    "# Prediction function with optional tensorboard visualizations\n",
    "def predict(model, test_loader, device, writer=None, visualize_samples=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    inference_start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            past, mask = [x.to(device) for x in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(past, mask)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            \n",
    "            # Visualize sample predictions (only if requested and writer is provided)\n",
    "            if writer is not None and visualize_samples and batch_idx < 10:\n",
    "                # In a real implementation, you would generate and save figures\n",
    "                # writer.add_figure(f'test/trajectory_{batch_idx}', fig, 0)\n",
    "                pass\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    \n",
    "    # Log inference statistics if writer is provided\n",
    "    if writer is not None:\n",
    "        inference_time = time.time() - inference_start_time\n",
    "        avg_time_per_sample = inference_time / len(predictions)\n",
    "        writer.add_text('inference_stats', \n",
    "                      f\"Total inference time: {inference_time:.2f}s, \"\n",
    "                      f\"Samples: {len(predictions)}, \"\n",
    "                      f\"Avg time per sample: {avg_time_per_sample*1000:.2f}ms\")\n",
    "        \n",
    "        # Add histogram of prediction coordinates\n",
    "        writer.add_histogram('test/pred_x', predictions[:, :, 0].flatten(), 0)\n",
    "        writer.add_histogram('test/pred_y', predictions[:, :, 1].flatten(), 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e14e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = 'data/train.npz'\n",
    "test_input  = 'data/test_input.npz'\n",
    "output_csv  = 'predictions.csv'\n",
    "\n",
    "timestamp         = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir           = f\"runs/simpl_{timestamp}\"\n",
    "ckpt_dir          = os.path.join(\"checkpoints\", f\"simpl_{timestamp}\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "best_ckpt_path    = os.path.join(ckpt_dir, \"best_model.pt\")\n",
    "periodic_tpl      = os.path.join(ckpt_dir, \"epoch_{:04d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe667e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "epochs = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca8ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Log hyperparameters\n",
    "hparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': lr,\n",
    "    'epochs': epochs,\n",
    "    'model_type': 'SIMPL',\n",
    "    'd_model': 128,\n",
    "    'nhead': 8,\n",
    "    'num_layers_temporal': 2,\n",
    "    'num_layers_social': 2,\n",
    "    'polynomial_degree': 5,\n",
    "    'dropout': 0.1,\n",
    "    'weight_decay': 1e-5,\n",
    "}\n",
    "writer.add_text('hyperparameters', str(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c4275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "print(\"Loading data...\")\n",
    "full_data = np.load(train_input)['data']\n",
    "\n",
    "# Split into train and eval (7:3)\n",
    "num_samples = len(full_data)\n",
    "num_train = int(0.7 * num_samples)\n",
    "perm = np.random.permutation(num_samples)\n",
    "train_idx = perm[:num_train]\n",
    "eval_idx = perm[num_train:]\n",
    "\n",
    "train_data = full_data[train_idx]\n",
    "eval_data = full_data[eval_idx]\n",
    "\n",
    "train_ds = TrajectoryDataset(data=train_data)\n",
    "eval_ds = TrajectoryDataset(data=eval_data)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(eval_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_ds = TrajectoryDataset(test_input, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Log dataset information\n",
    "writer.add_text('dataset_info', f\"Train samples: {len(train_ds)}, Eval samples: {len(eval_ds)}, Test samples: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d926097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create SIMPL model\n",
    "print(f\"Creating model on device: {device}\")\n",
    "model = SIMPLModel(\n",
    "    feature_dim=6,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers_temporal=2,\n",
    "    num_layers_social=2,\n",
    "    dim_feedforward=256,\n",
    "    T_past=50,\n",
    "    T_future=60,\n",
    "    polynomial_degree=5,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Log model architecture and parameters\n",
    "writer.add_text('model_architecture', str(model))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "writer.add_text('model_params', f\"Total trainable parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db2b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3b6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Resuming from epoch 100 (val_loss=271829.493948)\n"
     ]
    }
   ],
   "source": [
    "start_epoch   = 1\n",
    "best_val_loss = float('inf')\n",
    "global_step   = 0\n",
    "if os.path.exists(best_ckpt_path):\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    start_epoch   = ckpt['epoch'] + 1\n",
    "    best_val_loss = ckpt.get('val_loss', best_val_loss)\n",
    "    global_step   = ckpt.get('global_step', 0)\n",
    "    print(f\"▶ Resuming from epoch {ckpt['epoch']} (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Log the training loop start\n",
    "writer.add_text('training_info', f\"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 289.442177, Time: 14.75s\n",
      "Epoch 101/1000 | Train Loss: 356050.780979 | Val ADE: 373.6232 | Val FDE: 373.2393 | Time: 16.49s\n",
      "Training - Loss: 283.786454, Time: 14.05s\n",
      "Epoch 102/1000 | Train Loss: 327957.332135 | Val ADE: 303.4437 | Val FDE: 304.8571 | Time: 15.80s\n",
      "Training - Loss: 285.559433, Time: 14.06s\n",
      "Epoch 103/1000 | Train Loss: 349915.540417 | Val ADE: 340.9776 | Val FDE: 336.2518 | Time: 15.81s\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "try:\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train for one epoch\n",
    "        model, global_step = train(\n",
    "            model, train_loader, optimizer, device, \n",
    "            num_epochs=1, lr_scheduler=lr_scheduler,\n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(\n",
    "            model, eval_loader, device, \n",
    "            writer=writer, global_step=global_step\n",
    "        )\n",
    "        val_loss = val_metrics['loss']\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "                f\"Train Loss: {val_metrics['loss']:.6f} | \"\n",
    "                f\"Val ADE: {val_metrics['ade']:.4f} | \"\n",
    "                f\"Val FDE: {val_metrics['fde']:.4f} | \"\n",
    "                f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'global_step': global_step,\n",
    "                'hparams': hparams\n",
    "            }, best_ckpt_path)\n",
    "            print(f\"✅ Best model saved at epoch {epoch} (val loss: {best_val_loss:.6f})\")\n",
    "            writer.add_text('checkpoints', f\"New best model at epoch {epoch} with val_loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint_file = f'checkpoints/simpl_ckpt_epoch_{epoch:04d}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'global_step': global_step,\n",
    "                'hparams': hparams\n",
    "            }, checkpoint_file)\n",
    "            print(f\"🧪 Checkpoint saved at {checkpoint_file}\")\n",
    "            writer.add_text('checkpoints', f\"Periodic checkpoint at epoch {epoch}\")\n",
    "        \n",
    "        # # Early stopping check - if no improvement for 100 epochs\n",
    "        # if epoch > start_epoch + 100 and val_loss > best_val_loss * 0.99:\n",
    "        #     print(f\"Early stopping triggered. No significant improvement for 100 epochs.\")\n",
    "        #     writer.add_text('training_info', f\"Early stopping at epoch {epoch}\")\n",
    "        #     break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")\n",
    "    writer.add_text('training_info', f\"Training interrupted at epoch {epoch}\")\n",
    "\n",
    "finally:\n",
    "    # Calculate total training time\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
    "    writer.add_text('training_info', f\"Training completed/interrupted after {total_training_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_ckpt_path, map_location=device,weights_only=False)['model_state_dict'])\n",
    "preds = predict(model, test_loader, device)\n",
    "np.savetxt(output_csv, preds.reshape(-1, 2), delimiter=',')\n",
    "print(f\"Predictions saved to {output_csv}\")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
